{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**Maestría en Inteligencia Artificial Aplicada**\n",
        "##**Curso: Procesamiento de Lenguaje Natural (NLP)**\n",
        "###Tecnológico de Monterrey\n",
        "###Prof Luis Eduardo Falcón Morales\n",
        "\n",
        "## **Adtividad de la Semana 02**\n",
        "###**Introducción al procesamiento de texto.**"
      ],
      "metadata": {
        "id": "759SG4TyfbUn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En esta actividad deberás utilizar los datos del siguiente archivo que se encuentra en Canvas:\n",
        "\n",
        "MNA_NLP_semana_02_Actividad_datos.txt\n",
        "\n",
        "El archivo contiene comentarios en inglés sobre servicios de comida de la página de Yelp: https://www.yelp.com/ .\n",
        "\n",
        "Son mil comentarios y forman parte del conjunto de datos que se encuentra en el Machine Learning Repository de la UCI, llamado \"Sentiment Labelled Sentences\": https://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences#\n"
      ],
      "metadata": {
        "id": "6ue1YAKx3XDo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Parte 1. Cargamos los datos.**   "
      ],
      "metadata": {
        "id": "Zj-h4drXD-X9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cargar los datos del archivo indicado y obtener una lista de longitud de 1000 strings/comentarios.\n",
        "\n",
        "Por el momento solamente requerimos las bibliotecas de Numpy y re, para el manejo de los arreglos y de las expresiones regulares en Python.\n",
        "\n",
        "En particular, no necesitarás en esta actividad la biblioteca de Pandas.\n",
        "\n",
        "###**NOTA: En esta actividad no debes importar nada más, con estas dos bibliotecas será *suficiente*.**"
      ],
      "metadata": {
        "id": "BY6yifxscfrx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np       # importamos Numpy para el manejo de los arreglos.\n",
        "import re                # importamos re para el manejo de las expresiones regulares.\n",
        "import gdown             # importamos gdown para descargar el archivo de trabajo\n",
        "import os                # importamos os para manejo de archivos y directorios\n",
        "import tabulate as tab   # importamos tabulate para impresión de tablas"
      ],
      "metadata": {
        "id": "OJ26dAfhdFnf"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs('datos', exist_ok=True)\n",
        "datos_url = 'https://drive.google.com/uc?id=1qVQ5sn901fDCQMRP_NpaHEoTEEOlrfM3'\n",
        "output = 'datos/actividad_datos.txt'\n",
        "\n",
        "print(\"Descargando datos\")\n",
        "gdown.download(datos_url, output, quiet=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wqc0R1HMRP-x",
        "outputId": "b5124e4e-4bca-452f-d2f4-81c80aca36f2"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Descargando datos\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1qVQ5sn901fDCQMRP_NpaHEoTEEOlrfM3\n",
            "To: /home/jarcos/datos/actividad_datos.txt\n",
            "100%|█████████████████████████████████████████████████████████████████████| 59.9k/59.9k [00:00<00:00, 500kB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'datos/actividad_datos.txt'"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ejecuta las siguientes instrucciones para cargar la información del achivo dado:\n",
        "\n",
        "with open('datos/actividad_datos.txt',        # puedes actualizar la ruta a tu archivo, en dado caso.\n",
        "          mode='r',                           # abrimos el archivo en modo lectura.\n",
        "          ) as f:\n",
        "    docs = f.readlines()                      # separamos cada comentario por líneas\n",
        "\n",
        "f.close()                         # ya que tenemos la información en la variable docs, cerramos el archivo"
      ],
      "metadata": {
        "id": "QHUmJyjDdGNP"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(docs) == list   # Verifica que tu variable \"docs\" es una lista"
      ],
      "metadata": {
        "id": "L6WzrSrodG-Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30440842-79c4-41a0-b43c-fcebfb1e2799"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(docs)==1000  # verifica que la longitud de \"docs\" es de mil comentarios."
      ],
      "metadata": {
        "id": "QIK1u9WS2FtS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2efb80c-fcad-4032-b82b-27f853e2ca56"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "docs[0:10]     # observa algunos de los primeros comentarios"
      ],
      "metadata": {
        "id": "9AMLIfQvJqNZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b50c68db-020a-48dc-bf1d-113deb96c863"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Wow... Loved this place.\\n',\n",
              " 'Crust is not good.\\n',\n",
              " 'Not tasty and the texture was just nasty.\\n',\n",
              " 'Stopped by during the late May bank holiday off Rick Steve recommendation and loved it.\\n',\n",
              " 'The selection on the menu was great and so were the prices.\\n',\n",
              " 'Now I am getting angry and I want my damn pho.\\n',\n",
              " \"Honeslty it didn't taste THAT fresh.)\\n\",\n",
              " 'The potatoes were like rubber and you could tell they had been made up ahead of time being kept under a warmer.\\n',\n",
              " 'The fries were great too.\\n',\n",
              " 'A great touch.\\n']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Parte 2: sección de preguntas (regex).**   \n"
      ],
      "metadata": {
        "id": "k_ewoagic5jc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Instrucciones:**\n",
        "\n",
        "###**A continuación deberás contestar cada una de las preguntas que te piden usando expresiones regulares (regex).**\n",
        "\n",
        "###**Por el momento no hay restricción en cuanto al número de líneas de código que agregues, pero trata de incluir las mínimas posibles.**"
      ],
      "metadata": {
        "id": "X-eMJa3DFCIV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   **Pregunta 1.**\n",
        "\n",
        "Busca y elimina todos los saltos de línea '\\n' que se encuentran al final de cada comentario.\n",
        "\n",
        "Una vez finalizado, imprime los primeros 10 comentarios del resultado obtenido.\n"
      ],
      "metadata": {
        "id": "78nJMemzn5a5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "comentarios_sin_saltos = [re.sub(r'\\n$', '', registro) for registro in docs]\n",
        "for comentario in comentarios_sin_saltos[:10]:\n",
        "    print(comentario)"
      ],
      "metadata": {
        "id": "PwbYYIuZn8pE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4beec59-4317-4d9c-d454-700bbf6f11f5"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wow... Loved this place.\n",
            "Crust is not good.\n",
            "Not tasty and the texture was just nasty.\n",
            "Stopped by during the late May bank holiday off Rick Steve recommendation and loved it.\n",
            "The selection on the menu was great and so were the prices.\n",
            "Now I am getting angry and I want my damn pho.\n",
            "Honeslty it didn't taste THAT fresh.)\n",
            "The potatoes were like rubber and you could tell they had been made up ahead of time being kept under a warmer.\n",
            "The fries were great too.\n",
            "A great touch.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   **Pregunta 2.**  \n",
        "\n",
        "Busca e imprime todas las palabras que terminan con dos o más signos de admiración seguidos, por ejemplo \"!!!\".\n",
        "\n",
        "Debes imprimir tanto la palabra como la totalidad de signos de admiración que le siguen.\n",
        "\n",
        "Indica cuántos resultados obtuviste.\n",
        "\n"
      ],
      "metadata": {
        "id": "VWeKQC93ctEo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def imprimir_resultado(palabras,columnas):\n",
        "  filas = []\n",
        "  ordenados = sorted(palabras)\n",
        "  for i in range(0, len(ordenados), columnas):\n",
        "        fila = ordenados[i:i + columnas]\n",
        "\n",
        "        while len(fila) < columnas:\n",
        "            fila.append(\"\")\n",
        "        filas.append(fila)\n",
        "\n",
        "  print(tab.tabulate(filas, tablefmt=\"grid\", stralign=\"center\"))\n",
        "  print(f\"Se identificaron un total de \\033[32m\\033[1m{len(palabras)}\\033[0m palabras\")"
      ],
      "metadata": {
        "id": "ixtW-jri6upD"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "palabras_con_exclamaciones = []\n",
        "for comentario in comentarios_sin_saltos:\n",
        "    palabras_con_exclamaciones.extend(re.findall(r'\\b\\w+!{2,}', comentario))\n",
        "\n",
        "imprimir_resultado(palabras_con_exclamaciones,3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LwO2Gz0Y7vs1",
        "outputId": "019f3773-429b-4683-a2af-29c45d07b89d"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------+----------------------------+---------------------+\n",
            "| APPETIZERS!!!  |        DELICIOUS!!         |     FLY!!!!!!!!     |\n",
            "+----------------+----------------------------+---------------------+\n",
            "| Firehouse!!!!! |            Up!!            |      amazing!!      |\n",
            "+----------------+----------------------------+---------------------+\n",
            "|   amazing!!!   | amazing!!!!!!!!!!!!!!!!!!! |      awesome!!      |\n",
            "+----------------+----------------------------+---------------------+\n",
            "|  biscuits!!!   |         buffet!!!          |    delicious!!!     |\n",
            "+----------------+----------------------------+---------------------+\n",
            "| disappointed!! |      disappointing!!!      |        dry!!        |\n",
            "+----------------+----------------------------+---------------------+\n",
            "|     far!!      |           good!!           | great!!!!!!!!!!!!!! |\n",
            "+----------------+----------------------------+---------------------+\n",
            "|    here!!!     |           it!!!!           |     otherwise!!     |\n",
            "+----------------+----------------------------+---------------------+\n",
            "|    packed!!    |    shawarrrrrrma!!!!!!     |     steak!!!!!      |\n",
            "+----------------+----------------------------+---------------------+\n",
            "|    style!!     |          yucky!!!          |                     |\n",
            "+----------------+----------------------------+---------------------+\n",
            "Se identificaron un total de \u001b[32m\u001b[1m26\u001b[0m palabras\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   **Pregunta 3.**  \n",
        "\n",
        "Busca e imprime todas las palabras que están escritas totalmente en mayúsculas. Cada coincidencia debe ser una sola palabra.\n",
        "\n",
        "Indica cuántas palabras encontraste.\n",
        "\n"
      ],
      "metadata": {
        "id": "-s3okBqL96TT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "set_mayusculas = set()\n",
        "for comentario in comentarios_sin_saltos:\n",
        "    set_mayusculas.update(re.findall(r'\\b[A-Z]+\\b', comentario))\n",
        "\n",
        "palabras_mayusculas = list(set_mayusculas)\n",
        "\n",
        "imprimir_resultado(palabras_mayusculas,6)"
      ],
      "metadata": {
        "id": "yKHJkZKo_nW5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97123627-2d0a-4673-fc84-6ce45ae6a7a0"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+---------------+---------------+------------+-----------+------------+\n",
            "|     A     |     AGAIN     |      ALL      |     AN     |    AND    | APPETIZERS |\n",
            "+-----------+---------------+---------------+------------+-----------+------------+\n",
            "|   AVOID   |     AYCE      |      AZ       |    BACK    |   BARE    |  BARGAIN   |\n",
            "+-----------+---------------+---------------+------------+-----------+------------+\n",
            "|    BBQ    |     BEST      |    BETTER     |  BITCHES   |   BLAND   | CONCLUSION |\n",
            "+-----------+---------------+---------------+------------+-----------+------------+\n",
            "| DELICIOUS | ESTABLISHMENT |     EVER      | EXPERIENCE | FANTASTIC |   FLAVOR   |\n",
            "+-----------+---------------+---------------+------------+-----------+------------+\n",
            "|    FLY    |    FORWARD    |   FREEZING    |     FS     |    GC     |     GO     |\n",
            "+-----------+---------------+---------------+------------+-----------+------------+\n",
            "|   GREAT   |      HAD      |     HANDS     |  HAPPENED  |   HAVE    |    HOUR    |\n",
            "+-----------+---------------+---------------+------------+-----------+------------+\n",
            "|     I     |      IN       | INCONSIDERATE |     IT     |   LEGIT   |   LOVED    |\n",
            "+-----------+---------------+---------------+------------+-----------+------------+\n",
            "|     M     |  MANAGEMENT   |     MANY      |    MGM     |   MUST    |   NASTY    |\n",
            "+-----------+---------------+---------------+------------+-----------+------------+\n",
            "|   NEVER   |      NO       |     NONE      |    NOT     |    NOW    |    NYC     |\n",
            "+-----------+---------------+---------------+------------+-----------+------------+\n",
            "|    OF     |      OK       |      OMG      | OVERPRICED |  OWNERS   |   PEOPLE   |\n",
            "+-----------+---------------+---------------+------------+-----------+------------+\n",
            "|  PERFECT  |     REAL      |    REALLY     |     RI     |   RUDE    |  SCREAMS   |\n",
            "+-----------+---------------+---------------+------------+-----------+------------+\n",
            "|  SHOULD   |     STALE     |     STEP      |     T      |   THAT    |    THE     |\n",
            "+-----------+---------------+---------------+------------+-----------+------------+\n",
            "|   THIS    |     TIME      |     TOLD      |   TOTAL    |    TV     |   UNREAL   |\n",
            "+-----------+---------------+---------------+------------+-----------+------------+\n",
            "|   VERY    |     WASTE     |      WAY      |    WEAK    |   WHAT    |    WILL    |\n",
            "+-----------+---------------+---------------+------------+-----------+------------+\n",
            "|   WORST   |               |               |            |           |            |\n",
            "+-----------+---------------+---------------+------------+-----------+------------+\n",
            "Se identificaron un total de \u001b[32m\u001b[1m85\u001b[0m palabras\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   **Pregunta 4.**  \n",
        "\n",
        "Busca e imprime los comentarios en donde todos los caracteres alfabéticos (letras) están en mayúsculas.\n",
        "\n",
        "Cada coincidencia encontrada debe ser todo el comentario/enunciado.\n",
        "\n",
        "Indica cuántos resultados obtuviste.\n"
      ],
      "metadata": {
        "id": "GX8eYyDoMZma"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "comentarios_mayusculas = []\n",
        "\n",
        "for comentario in comentarios_sin_saltos:\n",
        "    if re.fullmatch(r'[A-Z\\s\\W]+', comentario):\n",
        "        comentarios_mayusculas.append(comentario)\n",
        "\n",
        "imprimir_resultado(comentarios_mayusculas,1)"
      ],
      "metadata": {
        "id": "K8VuZxvTMYj6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b8bc108-5ab4-4d62-ae3e-109b4b6f9d5f"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------------------------------------------------------------+\n",
            "|                      AVOID THIS ESTABLISHMENT!                       |\n",
            "+----------------------------------------------------------------------+\n",
            "|                             DELICIOUS!!                              |\n",
            "+----------------------------------------------------------------------+\n",
            "|                   RUDE & INCONSIDERATE MANAGEMENT.                   |\n",
            "+----------------------------------------------------------------------+\n",
            "|                         TOTAL WASTE OF TIME.                         |\n",
            "+----------------------------------------------------------------------+\n",
            "| WILL NEVER EVER GO BACK AND HAVE TOLD MANY PEOPLE WHAT HAD HAPPENED. |\n",
            "+----------------------------------------------------------------------+\n",
            "Se identificaron un total de \u001b[32m\u001b[1m5\u001b[0m palabras\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   **Pregunta 5.**  \n",
        "\n",
        "Busca e imprime todas las palabras que tengan una vocal acentuada, del tipo á, é, í, ó, ú.\n",
        "\n",
        "Indica cuántos resultados obtuviste."
      ],
      "metadata": {
        "id": "a1i6qv7-McmU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "palabras_tilde = []\n",
        "\n",
        "for comentario in comentarios_sin_saltos:\n",
        "    palabras_tilde.extend(re.findall(r'\\b\\w*[áéíóú]\\w*\\b', comentario, re.IGNORECASE))\n",
        "\n",
        "imprimir_resultado(palabras_tilde,3)"
      ],
      "metadata": {
        "id": "nZZ5zKUOMeGD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fcc83f78-1536-4f6a-fab1-ec88a08f26ce"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+--------+--------+\n",
            "| Café | fiancé | puréed |\n",
            "+------+--------+--------+\n",
            "Se identificaron un total de \u001b[32m\u001b[1m3\u001b[0m palabras\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   **Pregunta 6.**  \n",
        "\n",
        "Busca e imprime todas las cantidades numéricas monetarias, enteras o con decimales, que inician con el símbolo $\\$$.\n",
        "\n",
        "Indica cuántos resultados obtuviste."
      ],
      "metadata": {
        "id": "ZmPiAI82Mfb3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cantidades_monetarias = []\n",
        "\n",
        "for comentario in comentarios_sin_saltos:\n",
        "    cantidades_monetarias.extend(re.findall(r'\\$\\s*(\\d+(?:\\.\\d+)?)', comentario))\n",
        "\n",
        "imprimir_resultado(cantidades_monetarias,4)"
      ],
      "metadata": {
        "id": "6vhe9-Y-MhL9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9c88959-ab77-430c-a441-ca10573626bf"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----+----+-------+\n",
            "| 11.99 | 12 | 17 | 20    |\n",
            "+-------+----+----+-------+\n",
            "|  3    | 35 |  4 |  7.85 |\n",
            "+-------+----+----+-------+\n",
            "Se identificaron un total de \u001b[32m\u001b[1m8\u001b[0m palabras\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   **Pregunta 7.**  \n",
        "\n",
        "Busca e imprime todas las palabras que sean variantes de la palabra \"love\", sin importar si incluyen mayúsculas o minúsculas, o la manera en que esté conjugada o alguna otra variación que se haga con dicha palabra.\n",
        "\n",
        "Indica cuántos resultados obtuviste."
      ],
      "metadata": {
        "id": "2j-HpvhwMhq2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "regex = re.compile(r'\\b[Ll]ov(?:e[sd]?|ing|er|ers|ely|able|es|ed|ing|ingly|esque|ish|less|ness|ful|ment|s|d)\\b', re.IGNORECASE)\n",
        "palabras_love = []\n",
        "\n",
        "for comentario in comentarios_sin_saltos:\n",
        "  palabras_love.extend(regex.findall(comentario))\n",
        "\n",
        "imprimir_resultado(palabras_love,6)"
      ],
      "metadata": {
        "id": "kqqyRChVMjol",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c501dacc-aaf6-4450-91f2-f6e2cdc7691e"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+-------+--------+--------+-------+--------+\n",
            "| LOVED  | LOVED |  Love  |  Love  | Loved | Loved  |\n",
            "+--------+-------+--------+--------+-------+--------+\n",
            "|  love  | love  |  love  |  love  | love  |  love  |\n",
            "+--------+-------+--------+--------+-------+--------+\n",
            "|  love  | love  |  love  |  love  | love  |  love  |\n",
            "+--------+-------+--------+--------+-------+--------+\n",
            "|  love  | love  |  love  |  love  | love  | loved  |\n",
            "+--------+-------+--------+--------+-------+--------+\n",
            "| loved  | loved | loved  | loved  | loved | lovely |\n",
            "+--------+-------+--------+--------+-------+--------+\n",
            "| lovely | lover | lovers | lovers | loves | loving |\n",
            "+--------+-------+--------+--------+-------+--------+\n",
            "Se identificaron un total de \u001b[32m\u001b[1m36\u001b[0m palabras\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   **Pregunta 8.**  \n",
        "\n",
        "Busca e imprime todas las palabras, variantes de \"so\" y \"good\", que tengan dos o más \"o\" en \"so\" y 3 o más \"o\" en good.\n",
        "\n",
        "Indica cuántas encontraste.\n"
      ],
      "metadata": {
        "id": "Ctb-NTY3MkYG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "regex_so = re.compile(r'\\bso+o+\\b', re.IGNORECASE)\n",
        "regex_good = re.compile(r'\\bgo+o+od\\w*\\b', re.IGNORECASE)\n",
        "\n",
        "# Buscar y almacenar todas las variantes de \"so\" y \"good\"\n",
        "palabras_so = []\n",
        "palabras_good = []\n",
        "for comentario in comentarios_sin_saltos:\n",
        "    palabras_so.extend(regex_so.findall(comentario))\n",
        "    palabras_good.extend(regex_good.findall(comentario))\n",
        "\n",
        "imprimir_resultado(palabras_so,3)\n",
        "imprimir_resultado(palabras_good,1)"
      ],
      "metadata": {
        "id": "A8Nf3B_cMlqg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e6d2c42-6211-4043-dade-b94ffa953340"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-------+-------+\n",
            "| Sooooo  | soooo | soooo |\n",
            "+---------+-------+-------+\n",
            "| soooooo |       |       |\n",
            "+---------+-------+-------+\n",
            "Se identificaron un total de \u001b[32m\u001b[1m4\u001b[0m palabras\n",
            "+--------+\n",
            "| gooodd |\n",
            "+--------+\n",
            "Se identificaron un total de \u001b[32m\u001b[1m1\u001b[0m palabras\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   **Pregunta 9.**  \n",
        "\n",
        "Busca e imprime todas las palabras que tengan una longitud mayor estrictamente a 10 caracteres alfabéticos.\n",
        "\n",
        "No se consideran los signos de puntuación o caracteres especiales en la longitud de estas cadenas, solo caracteres alfabéticos en mayúsculas o minúsculas.\n",
        "\n",
        "Indica la cantidad de palabras encontradas.\n"
      ],
      "metadata": {
        "id": "hkak1opjMmlk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "regex = re.compile(r'\\b[a-zA-Z]{11,}\\b')\n",
        "palabras_largas = []\n",
        "for comentario in comentarios_sin_saltos:\n",
        "  palabras_largas.extend(regex.findall(comentario))\n",
        "\n",
        "imprimir_resultado(palabras_largas,7)"
      ],
      "metadata": {
        "id": "PYxdp3uhMoD0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "831abb69-6ad9-4cdb-b059-ac9d1b58e621",
        "collapsed": true
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+-----------------+----------------+----------------+----------------+----------------+-----------------+\n",
            "|   Disappointed    |  Disappointing  | ESTABLISHMENT  |  Furthermore   | INCONSIDERATE  |  Interesting   |  Mediterranean  |\n",
            "+-------------------+-----------------+----------------+----------------+----------------+----------------+-----------------+\n",
            "|    Outstanding    |  Philadelphia   |  Smashburger   | Unfortunately  | Unfortunately  | Unfortunately  |   Veggitarian   |\n",
            "+-------------------+-----------------+----------------+----------------+----------------+----------------+-----------------+\n",
            "| WAAAAAAyyyyyyyyyy | Wienerschnitzel | accommodations |  accordingly   |  acknowledged  |  acknowledged  |   anticipated   |\n",
            "+-------------------+-----------------+----------------+----------------+----------------+----------------+-----------------+\n",
            "|    beautifully    |   calligraphy   |  caterpillar   |  cheeseburger  |  cheeseburger  |  cheesecurds   |  circumstances  |\n",
            "+-------------------+-----------------+----------------+----------------+----------------+----------------+-----------------+\n",
            "|    combination    |   combination   |  comfortable   |  comfortable   |  compliments   |  connoisseur   |   considering   |\n",
            "+-------------------+-----------------+----------------+----------------+----------------+----------------+-----------------+\n",
            "|    considering    |   considering   |  considering   |  constructed   |  corporation   |  deliciously   |  descriptions   |\n",
            "+-------------------+-----------------+----------------+----------------+----------------+----------------+-----------------+\n",
            "|   deuchebaggery   |  disappointed   |  disappointed  |  disappointed  |  disappointed  |  disappointed  |  disappointed   |\n",
            "+-------------------+-----------------+----------------+----------------+----------------+----------------+-----------------+\n",
            "|   disappointed    |  disappointed   |  disappointed  |  disappointed  |  disappointed  |  disappointed  |  disappointed   |\n",
            "+-------------------+-----------------+----------------+----------------+----------------+----------------+-----------------+\n",
            "|   disappointed    |  disappointed   |  disappointed  |  disappointed  |  disappointed  | disappointing  |  disappointing  |\n",
            "+-------------------+-----------------+----------------+----------------+----------------+----------------+-----------------+\n",
            "|   disappointing   |  disappointing  | disappointment | disappointment | disappointment | disappointment | disapppointment |\n",
            "+-------------------+-----------------+----------------+----------------+----------------+----------------+-----------------+\n",
            "|    disgraceful    |  disrespected   |  disrespected  |  drastically   |  enthusiastic  | establishment  |  establishment  |\n",
            "+-------------------+-----------------+----------------+----------------+----------------+----------------+-----------------+\n",
            "|   establishment   |   exceptional   |  expectations  |  experienced   |  experiencing  | extraordinary  |   grandmother   |\n",
            "+-------------------+-----------------+----------------+----------------+----------------+----------------+-----------------+\n",
            "|    highlighted    |   hospitality   |  imagination   |  imaginative   |  immediately   |  immediately   |   immediately   |\n",
            "+-------------------+-----------------+----------------+----------------+----------------+----------------+-----------------+\n",
            "|    inexpensive    |   inexpensive   |  informative   |  ingredients   |  interesting   |  interesting   |   maintaining   |\n",
            "+-------------------+-----------------+----------------+----------------+----------------+----------------+-----------------+\n",
            "|   neighborhood    |  neighborhood   |  opportunity   |  opportunity   |  outrageously  |  outstanding   |   outstanding   |\n",
            "+-------------------+-----------------+----------------+----------------+----------------+----------------+-----------------+\n",
            "|    overwhelmed    |   overwhelmed   |  presentation  |  presentation  |  professional  |  professional  |   profiterole   |\n",
            "+-------------------+-----------------+----------------+----------------+----------------+----------------+-----------------+\n",
            "|  recommendation   | recommendation  | recommendation |  recommended   |  recommended   |  recommended   |  recommending   |\n",
            "+-------------------+-----------------+----------------+----------------+----------------+----------------+-----------------+\n",
            "|   relationship    |   replenished   |  reservation   |  restaurants   |  restaurants   |  restaurants   |  shawarrrrrrma  |\n",
            "+-------------------+-----------------+----------------+----------------+----------------+----------------+-----------------+\n",
            "|    suggestions    |   traditional   |  transcendant  |  unbelievable  |  unbelievably  |  undercooked   |   undercooked   |\n",
            "+-------------------+-----------------+----------------+----------------+----------------+----------------+-----------------+\n",
            "|   underwhelming   |  underwhelming  | unexperienced  | unfortunately  | unprofessional |  unsatisfying  |   ventilation   |\n",
            "+-------------------+-----------------+----------------+----------------+----------------+----------------+-----------------+\n",
            "|    vinaigrette    |                 |                |                |                |                |                 |\n",
            "+-------------------+-----------------+----------------+----------------+----------------+----------------+-----------------+\n",
            "Se identificaron un total de \u001b[32m\u001b[1m141\u001b[0m palabras\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   **Pregunta 10.**  \n",
        "\n",
        "Busca e imprime todas las palabras que inician con una letra mayúscula y terminan con una minúscula, pero que además no sea la primera palabra del comentario/string.\n",
        "\n",
        "Indica la cantidad de resultados obtenidos."
      ],
      "metadata": {
        "id": "ApjTNzSxMpDc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "patron_10 = r\"(?<!^)\\b[A-Z][a-z]*[a-z]\\b\"\n",
        "palabras_10 = []\n",
        "for comentario in comentarios_sin_saltos:\n",
        "  palabras_10.extend(re.findall(patron_10, comentario))\n",
        "\n",
        "imprimir_resultado(palabras_10,9)"
      ],
      "metadata": {
        "id": "Vb0ndRGAMqdL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83ddc81c-6d26-46f2-ca2c-503031cd2010"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------+---------------+---------------+-----------------+------------+--------------+-----------+----------+---------------+\n",
            "|     After     |     After     |     After     |   Albondigas    |    All     |     All      |    All    |   All    |     Also      |\n",
            "+---------------+---------------+---------------+-----------------+------------+--------------+-----------+----------+---------------+\n",
            "|     Also      |     Also      |     Also      |      Also       |  Although  |     And      |    And    |  Anyway  |    Anyway     |\n",
            "+---------------+---------------+---------------+-----------------+------------+--------------+-----------+----------+---------------+\n",
            "|    Anyways    |      Are      |     Area      |      Aria       |     As     |      As      |    As     |    As    |      At       |\n",
            "+---------------+---------------+---------------+-----------------+------------+--------------+-----------+----------+---------------+\n",
            "|      At       |    Attack     |     Baba      |      Bachi      |   Bachi    |     Back     |    Bad    |   Bad    |      Bar      |\n",
            "+---------------+---------------+---------------+-----------------+------------+--------------+-----------+----------+---------------+\n",
            "|   Baseball    |      Bay      |      Bay      |       Bay       |    Bay     |      Be      |   Bean    | Bellagio |     Best      |\n",
            "+---------------+---------------+---------------+-----------------+------------+--------------+-----------+----------+---------------+\n",
            "|     Best      |     Best      |      Big      |     Bisque      |   Bisque   |     Blah     |  Bloody   |   Both   |     Both      |\n",
            "+---------------+---------------+---------------+-----------------+------------+--------------+-----------+----------+---------------+\n",
            "|    Bouchon    |    Breeze     |   Brushfire   |     Buffet      |   Buffet   |   Buldogis   |   Bunch   |  Burger  |   Burrittos   |\n",
            "+---------------+---------------+---------------+-----------------+------------+--------------+-----------+----------+---------------+\n",
            "|    Bussell    |      By       |   Caballero   |     Caesar      | Camelback  |     Cape     |   Carly   |  Cartel  |    Casino     |\n",
            "+---------------+---------------+---------------+-----------------+------------+--------------+-----------+----------+---------------+\n",
            "|    Chicken    |    Chicken    |    Chinese    |     Chinese     |  Chipotle  |  Christmas   |   Cibo    |  Classy  |      Cod      |\n",
            "+---------------+---------------+---------------+-----------------+------------+--------------+-----------+----------+---------------+\n",
            "|    Coffee     |     Come      |    Company    |     Costco      |   Cotta    |    Crema     | Crystals  | Customer |     Cute      |\n",
            "+---------------+---------------+---------------+-----------------+------------+--------------+-----------+----------+---------------+\n",
            "|  Definitely   |   Delicious   |    Delight    |      Denny      |  Despite   | Disappointed |    Dog    |   Dos    |    Drinks     |\n",
            "+---------------+---------------+---------------+-----------------+------------+--------------+-----------+----------+---------------+\n",
            "|     Dylan     |   Edinburgh   |      Egg      |    Eggplant     |    Elk     |   English    |    Eve    |  Every   |   Everyone    |\n",
            "+---------------+---------------+---------------+-----------------+------------+--------------+-----------+----------+---------------+\n",
            "|  Everything   |   Excalibur   |  Experience   |      Filet      |   Filet    |  Firehouse   |  Flower   |  Flower  |     Food      |\n",
            "+---------------+---------------+---------------+-----------------+------------+--------------+-----------+----------+---------------+\n",
            "|     Food      |      For      |      For      |       For       |    For     |  Francisco   | Frenchman | Fridays  |    Friend     |\n",
            "+---------------+---------------+---------------+-----------------+------------+--------------+-----------+----------+---------------+\n",
            "|    Frozen     |  Furthermore  |    Ganoush    |      Give       |    Gold    |     Good     |   Good    |   Good   |    Gordon     |\n",
            "+---------------+---------------+---------------+-----------------+------------+--------------+-----------+----------+---------------+\n",
            "|    Gourmet    |     Great     |     Great     |      Great      |   Great    |    Great     |   Great   |  Great   |     Great     |\n",
            "+---------------+---------------+---------------+-----------------+------------+--------------+-----------+----------+---------------+\n",
            "|     Greek     |     Greek     |     Greek     |      Green      |   Grill    |   Gringos    |   Gyros   |    Ha    |    Halibut    |\n",
            "+---------------+---------------+---------------+-----------------+------------+--------------+-----------+----------+---------------+\n",
            "|      Han      |     Hard      |   Hawaiian    |       He        |     He     |    Heart     |  Heimer   |   Hiro   |   Honestly    |\n",
            "+---------------+---------------+---------------+-----------------+------------+--------------+-----------+----------+---------------+\n",
            "|     Host      |      Hot      |      Hot      |       Hot       |    How     |   However    |  However  |  Hunan   |      Hut      |\n",
            "+---------------+---------------+---------------+-----------------+------------+--------------+-----------+----------+---------------+\n",
            "|     Ians      |      If       |      If       |       If        |     If     |      If      |    If     |    If    |      If       |\n",
            "+---------------+---------------+---------------+-----------------+------------+--------------+-----------+----------+---------------+\n",
            "|      In       |      In       |      In       |       In        |     In     |    Indian    |  Indian   | Insults  |    Ironman    |\n",
            "+---------------+---------------+---------------+-----------------+------------+--------------+-----------+----------+---------------+\n",
            "|      It       |      It       |      It       |       It        |     It     |      It      |    It     |    It    |      It       |\n",
            "+---------------+---------------+---------------+-----------------+------------+--------------+-----------+----------+---------------+\n",
            "|      It       |    Italian    |    Italian    |    Jamaican     |  Japanese  |     Jeff     |   Jenni   |   Joey   |    Kabuki     |\n",
            "+---------------+---------------+---------------+-----------------+------------+--------------+-----------+----------+---------------+\n",
            "|     Khao      |     Large     |      Las      |     Lastly      |   Lemon    |    Level     |  Lobster  | Lobster  |    Lobster    |\n",
            "+---------------+---------------+---------------+-----------------+------------+--------------+-----------+----------+---------------+\n",
            "|     Long      |     Lordy     |     Love      |      Love       |   Loved    |    Loved     |    Lox    |   Luke   |   Macarons    |\n",
            "+---------------+---------------+---------------+-----------------+------------+--------------+-----------+----------+---------------+\n",
            "|    Madison    |   Magazine    |     Magic     |      Main       |   Maine    |   Mandalay   |   Mango   |  Maria   |     Mary      |\n",
            "+---------------+---------------+---------------+-----------------+------------+--------------+-----------+----------+---------------+\n",
            "|      May      |     Maybe     | Mediterranean |     Mellow      |  Mexican   |   Mexican    |  Mirage   |   Mmmm   |      Mom      |\n",
            "+---------------+---------------+---------------+-----------------+------------+--------------+-----------+----------+---------------+\n",
            "|   Mushroom    |      My       |      My       |       My        |     My     |     Nan      | Needless  | Needless |     Never     |\n",
            "+---------------+---------------+---------------+-----------------+------------+--------------+-----------+----------+---------------+\n",
            "|     Nice      |     Nice      |     Ninja     |       No        |     No     |     Nobu     |   Noca    |  North   |      Not      |\n",
            "+---------------+---------------+---------------+-----------------+------------+--------------+-----------+----------+---------------+\n",
            "|      Not      |      Not      |      Not      |       Not       |    Now     |      Of      |    Oh     |    On    |      On       |\n",
            "+---------------+---------------+---------------+-----------------+------------+--------------+-----------+----------+---------------+\n",
            "|      On       |      On       |      On       |       One       |  Ordered   |     Otto     |    Our    |   Our    |      Our      |\n",
            "+---------------+---------------+---------------+-----------------+------------+--------------+-----------+----------+---------------+\n",
            "|      Our      |  Outstanding  |    Overall    |     Overall     |  Overall   |   Overall    |  Overall  |   Palm   |     Panna     |\n",
            "+---------------+---------------+---------------+-----------------+------------+--------------+-----------+----------+---------------+\n",
            "|   Paradise    |    Perfect    |    Perfect    |     Perhaps     | Phenomenal | Philadelphia |    Pho    |   Pho    |      Pho      |\n",
            "+---------------+---------------+---------------+-----------------+------------+--------------+-----------+----------+---------------+\n",
            "|      Pho      |    Phoenix    |    Phoenix    |     Phoenix     |  Phoenix   |  Pineapple   |   Pita    |  Pizza   |     Pizza     |\n",
            "+---------------+---------------+---------------+-----------------+------------+--------------+-----------+----------+---------------+\n",
            "|     Place     |    Plater     |     Plus      |      Point      |    Poor    |    Prices    |  Prices   | Probably |     Pros      |\n",
            "+---------------+---------------+---------------+-----------------+------------+--------------+-----------+----------+---------------+\n",
            "|    Ramsey     |    Really     |     Rice      |      Rick       |  Risotto   |     Rock     |   Roll    |  Sadly   |     Salad     |\n",
            "+---------------+---------------+---------------+-----------------+------------+--------------+-----------+----------+---------------+\n",
            "|    Salads     |     Same      |      San      |       Sat       | Scottsdale |     Seat     | Seriously | Service  |    Service    |\n",
            "+---------------+---------------+---------------+-----------------+------------+--------------+-----------+----------+---------------+\n",
            "|    Service    |     Shop      |   Similarly   |   Smashburger   |    Soi     |    Sorry     |   Soups   |   Sour   |    Sprouts    |\n",
            "+---------------+---------------+---------------+-----------------+------------+--------------+-----------+----------+---------------+\n",
            "|   Standard    |     Stars     |     Steak     |      Steak      |  Steiners  |    Steve     |  Stopped  |  Strike  |     Strip     |\n",
            "+---------------+---------------+---------------+-----------------+------------+--------------+-----------+----------+---------------+\n",
            "|     Strip     |    Subway     |    Subway     |     Subway      |    Sun     |    Sunday    |   Sushi   |   Taco   |     Tasty     |\n",
            "+---------------+---------------+---------------+-----------------+------------+--------------+-----------+----------+---------------+\n",
            "|     Thai      |     Thai      |     Thai      |      Thai       |    Thai    |     That     |   That    |   The    |      The      |\n",
            "+---------------+---------------+---------------+-----------------+------------+--------------+-----------+----------+---------------+\n",
            "|      The      |      The      |      The      |       The       |    The     |     The      |    The    |   The    |      The      |\n",
            "+---------------+---------------+---------------+-----------------+------------+--------------+-----------+----------+---------------+\n",
            "|      The      |      The      |      The      |       The       |    The     |     The      |    The    |   The    |      The      |\n",
            "+---------------+---------------+---------------+-----------------+------------+--------------+-----------+----------+---------------+\n",
            "|      The      |      The      |      The      |       The       |    The     |     The      |    The    |   The    |      The      |\n",
            "+---------------+---------------+---------------+-----------------+------------+--------------+-----------+----------+---------------+\n",
            "|      The      |      The      |      The      |       The       |    The     |     The      |    The    |   The    |      The      |\n",
            "+---------------+---------------+---------------+-----------------+------------+--------------+-----------+----------+---------------+\n",
            "|      The      |      The      |      The      |       The       |    The     |     The      |    The    |   The    |      The      |\n",
            "+---------------+---------------+---------------+-----------------+------------+--------------+-----------+----------+---------------+\n",
            "|      The      |      The      |      The      |       The       |    The     |     The      |    The    |   The    |      The      |\n",
            "+---------------+---------------+---------------+-----------------+------------+--------------+-----------+----------+---------------+\n",
            "|     Their     |     Then      |     Then      |      There      |    They    |     They     |   They    |   They   |     They      |\n",
            "+---------------+---------------+---------------+-----------------+------------+--------------+-----------+----------+---------------+\n",
            "|     They      |     They      |     Third     |      This       |    This    |     This     |   This    |   This   |     This      |\n",
            "+---------------+---------------+---------------+-----------------+------------+--------------+-----------+----------+---------------+\n",
            "|     This      |     This      |     This      |      This       |    This    |     This     |  Thumbs   |   Thus   |  Tigerlilly   |\n",
            "+---------------+---------------+---------------+-----------------+------------+--------------+-----------+----------+---------------+\n",
            "|      To       |      To       |      To       |      Toast      |    Took    |    Total     |    Try    |  Tucson  | Unfortunately |\n",
            "+---------------+---------------+---------------+-----------------+------------+--------------+-----------+----------+---------------+\n",
            "| Unfortunately | Unfortunately |      Up       |     Valley      |   Valley   |    Vegas     |   Vegas   |  Vegas   |     Vegas     |\n",
            "+---------------+---------------+---------------+-----------------+------------+--------------+-----------+----------+---------------+\n",
            "|     Vegas     |     Vegas     |     Vegas     |      Vegas      |   Vegas    |    Vegas     |   Vegas   |  Vegas   |     Vegas     |\n",
            "+---------------+---------------+---------------+-----------------+------------+--------------+-----------+----------+---------------+\n",
            "|     Vegas     |     Vegas     |     Vegas     |      Vegas      |   Vegas    |    Vegas     |   Vegas   |  Vegas   |     Vegas     |\n",
            "+---------------+---------------+---------------+-----------------+------------+--------------+-----------+----------+---------------+\n",
            "|  Vegetarian   |  Veggitarian  |     Very      |      Very       |    Very    |     Very     |   Very    |   Very   |     Very      |\n",
            "+---------------+---------------+---------------+-----------------+------------+--------------+-----------+----------+---------------+\n",
            "|    Voodoo     |      We       |      We       |       We        |     We     |     Went     |   What    |   What   |     When      |\n",
            "+---------------+---------------+---------------+-----------------+------------+--------------+-----------+----------+---------------+\n",
            "|     When      |     When      |     When      | Wienerschnitzel |    Wife    |     Will     |   Worse   |  Worst   |     Yama      |\n",
            "+---------------+---------------+---------------+-----------------+------------+--------------+-----------+----------+---------------+\n",
            "|     Yeah      |    Yelpers    |      You      |      Your       |            |              |           |          |               |\n",
            "+---------------+---------------+---------------+-----------------+------------+--------------+-----------+----------+---------------+\n",
            "Se identificaron un total de \u001b[32m\u001b[1m517\u001b[0m palabras\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   **Pregunta 11.**  \n",
        "\n",
        "Busca e imprime la secuencia de dos o más palabras que están separadas por un guion, \"-\", sin que tengan espacios en blanco entre ellas.\n",
        "\n",
        "Por ejemplo \"Go-Kart\" sería válido, pero \"Go  -Kart\" o \"Go  -  Kart\" no lo serían.\n",
        "\n",
        "Indica la cantidad de resultados obtenidos."
      ],
      "metadata": {
        "id": "u7nfm4KhMrNW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "patron_11 = r\"\\b\\w+-\\w+\\b\"\n",
        "palabras_11 = []\n",
        "for comentario in comentarios_sin_saltos:\n",
        "  palabras_11.extend(re.findall(patron_11, comentario))\n",
        "\n",
        "imprimir_resultado(palabras_11,3)"
      ],
      "metadata": {
        "id": "OwU-a7eGMsub",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7bdc5f5-f182-47c9-a0bc-68b29f28ec6e"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------+--------------------+----------------+\n",
            "| High-quality |   Service-check    |  been-stepped  |\n",
            "+--------------+--------------------+----------------+\n",
            "|  flat-lined  |   golden-crispy    |   hands-down   |\n",
            "+--------------+--------------------+----------------+\n",
            "|    in-and    |      in-house      |    low-key     |\n",
            "+--------------+--------------------+----------------+\n",
            "| multi-grain  |     must-stop      |  non-customer  |\n",
            "+--------------+--------------------+----------------+\n",
            "|  non-fancy   |      over-hip      |  over-priced   |\n",
            "+--------------+--------------------+----------------+\n",
            "|  over-whelm  |      sit-down      |    sub-par     |\n",
            "+--------------+--------------------+----------------+\n",
            "|    to-go     | tracked-everywhere | under-services |\n",
            "+--------------+--------------------+----------------+\n",
            "Se identificaron un total de \u001b[32m\u001b[1m21\u001b[0m palabras\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   **Pregunta 12.**  \n",
        "\n",
        "Busca e imprime todas las palabras que terminan en \"ing\" o \"ed\".\n",
        "\n",
        "Indica la cantidad de palabras que encontraste de cada una."
      ],
      "metadata": {
        "id": "DEIgl79HMthr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "patron_ing = r\"\\b\\w+ing\\b\"\n",
        "patron_ed = r\"\\b\\w+ed\\b\"\n",
        "\n",
        "# Buscar palabras que coincidan con cada patrón\n",
        "palabras_ing = []\n",
        "palabras_ed = []\n",
        "for comentario in comentarios_sin_saltos:\n",
        "  palabras_ing.extend(re.findall(patron_ing, comentario))\n",
        "  palabras_ed.extend(re.findall(patron_ed, comentario))\n",
        "\n",
        "imprimir_resultado(palabras_ing,9)"
      ],
      "metadata": {
        "id": "I4TSofBMMv9y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e7b9888-8810-4645-da4d-7b9486dbf001"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------+---------------+---------------+---------------+---------------+---------------+--------------+-------------+-------------+\n",
            "|    Coming     | Disappointing |  Everything   |  Everything   |  Everything   |  Everything   |  Everything  | Everything  | Interesting |\n",
            "+---------------+---------------+---------------+---------------+---------------+---------------+--------------+-------------+-------------+\n",
            "|    Nothing    |  Outstanding  |    Paying     |    Pricing    |    amazing    |    amazing    |   amazing    |   amazing   |   amazing   |\n",
            "+---------------+---------------+---------------+---------------+---------------+---------------+--------------+-------------+-------------+\n",
            "|    amazing    |    amazing    |    amazing    |    amazing    |    amazing    |    amazing    |   amazing    |   amazing   |   amazing   |\n",
            "+---------------+---------------+---------------+---------------+---------------+---------------+--------------+-------------+-------------+\n",
            "|    amazing    |    amazing    |    amazing    |    amazing    |    amazing    |    amazing    |   amazing    |   amazing   |   amazing   |\n",
            "+---------------+---------------+---------------+---------------+---------------+---------------+--------------+-------------+-------------+\n",
            "|    amazing    |   annoying    |   anything    |   anything    |   appalling   |   appealing   |   arriving   |   asking    |    being    |\n",
            "+---------------+---------------+---------------+---------------+---------------+---------------+--------------+-------------+-------------+\n",
            "|     being     |     being     |     being     |     being     |     being     |     being     |    being     |    being    |    being    |\n",
            "+---------------+---------------+---------------+---------------+---------------+---------------+--------------+-------------+-------------+\n",
            "|     being     |     being     |     being     |     being     |    boring     |     bring     |    bring     |    bring    |    bring    |\n",
            "+---------------+---------------+---------------+---------------+---------------+---------------+--------------+-------------+-------------+\n",
            "|     bring     |   building    |   building    |    buying     |    caring     |   changing    |   charming   |  climbing   |   coming    |\n",
            "+---------------+---------------+---------------+---------------+---------------+---------------+--------------+-------------+-------------+\n",
            "|    coming     |    coming     |    coming     |    coming     |    coming     |    coming     |    coming    |   coming    | considering |\n",
            "+---------------+---------------+---------------+---------------+---------------+---------------+--------------+-------------+-------------+\n",
            "|  considering  |  considering  |  considering  |    cooking    |   cramming    |    craving    |   dealing    |   dealing   | describing  |\n",
            "+---------------+---------------+---------------+---------------+---------------+---------------+--------------+-------------+-------------+\n",
            "|    dining     |    dining     |    dining     |    dining     |    dining     |    dining     |    dining    |   dining    |   dipping   |\n",
            "+---------------+---------------+---------------+---------------+---------------+---------------+--------------+-------------+-------------+\n",
            "| disappointing | disappointing | disappointing | disappointing |  disgusting   |  disgusting   |    doing     |   drawing   |  dressing   |\n",
            "+---------------+---------------+---------------+---------------+---------------+---------------+--------------+-------------+-------------+\n",
            "|   dressing    |   dressing    |   drinking    |   dripping    |    driving    |    during     |    during    |   during    |   during    |\n",
            "+---------------+---------------+---------------+---------------+---------------+---------------+--------------+-------------+-------------+\n",
            "|    during     |    eating     |    eating     |    eating     |    eating     |    eating     |    eating    |   eating    |   eating    |\n",
            "+---------------+---------------+---------------+---------------+---------------+---------------+--------------+-------------+-------------+\n",
            "|    editing    |    evening    |    evening    |  everything   |  everything   |  everything   |  everything  | everything  | everything  |\n",
            "+---------------+---------------+---------------+---------------+---------------+---------------+--------------+-------------+-------------+\n",
            "|   exceeding   | experiencing  |    falling    |    feeling    |    feeling    |    filling    |   filling    |  flirting   | forgetting  |\n",
            "+---------------+---------------+---------------+---------------+---------------+---------------+--------------+-------------+-------------+\n",
            "|   freaking    |    fucking    |    getting    |    getting    |    getting    |    getting    |   getting    |   getting   |   getting   |\n",
            "+---------------+---------------+---------------+---------------+---------------+---------------+--------------+-------------+-------------+\n",
            "|    getting    |    giving     |     going     |     going     |     going     |     going     |    going     |    going    |    going    |\n",
            "+---------------+---------------+---------------+---------------+---------------+---------------+--------------+-------------+-------------+\n",
            "|     going     |     going     |     going     |     going     |     going     |     going     |    going     |    going    |    going    |\n",
            "+---------------+---------------+---------------+---------------+---------------+---------------+--------------+-------------+-------------+\n",
            "|     going     |     going     |   handling    |   hankering   |    having     |    hoping     |  including   | interesting | interesting |\n",
            "+---------------+---------------+---------------+---------------+---------------+---------------+--------------+-------------+-------------+\n",
            "|   inviting    |    judging    |    lacking    |    lacking    |    lacking    |    letting    |   lighting   |   liking    |   living    |\n",
            "+---------------+---------------+---------------+---------------+---------------+---------------+--------------+-------------+-------------+\n",
            "|    looking    |    looking    |    loving     |  maintaining  |    making     |    missing    |   nothing    |   nothing   |   nothing   |\n",
            "+---------------+---------------+---------------+---------------+---------------+---------------+--------------+-------------+-------------+\n",
            "|    nothing    |    nothing    |    nothing    |   ordering    |  outshining   |  outstanding  | outstanding  |   playing   |   playing   |\n",
            "+---------------+---------------+---------------+---------------+---------------+---------------+--------------+-------------+-------------+\n",
            "|   poisoning   |   preparing   |   preparing   |    pricing    |   providing   |    putting    |    rating    |   raving    |   reading   |\n",
            "+---------------+---------------+---------------+---------------+---------------+---------------+--------------+-------------+-------------+\n",
            "| recommending  |   redeeming   |  refreshing   |   returning   |   reviewing   |  revisiting   |   rotating   |   running   |   running   |\n",
            "+---------------+---------------+---------------+---------------+---------------+---------------+--------------+-------------+-------------+\n",
            "|    running    |    running    |   satifying   |  satisfying   |  satisfying   |    saving     |    saying    |  seasoning  |   seating   |\n",
            "+---------------+---------------+---------------+---------------+---------------+---------------+--------------+-------------+-------------+\n",
            "|    seating    |    seating    |    serving    |    serving    |    setting    |    setting    |   setting    |  shopping   |   sitting   |\n",
            "+---------------+---------------+---------------+---------------+---------------+---------------+--------------+-------------+-------------+\n",
            "|    sitting    |   something   |   something   |   something   |   something   |   sporting    |    spring    |  starving   |   staying   |\n",
            "+---------------+---------------+---------------+---------------+---------------+---------------+--------------+-------------+-------------+\n",
            "|    staying    |    talking    |     thing     |     thing     |     thing     |     thing     |    thing     |    thing    |    thing    |\n",
            "+---------------+---------------+---------------+---------------+---------------+---------------+--------------+-------------+-------------+\n",
            "|     thing     |   thinking    |    trying     |    trying     | underwhelming | underwhelming | unsatisfying |  upgrading  |  venturing  |\n",
            "+---------------+---------------+---------------+---------------+---------------+---------------+--------------+-------------+-------------+\n",
            "|    waiting    |    waiting    |    waiting    |    waiting    |    waiting    |    wasting    |   wasting    |   working   |   writing   |\n",
            "+---------------+---------------+---------------+---------------+---------------+---------------+--------------+-------------+-------------+\n",
            "Se identificaron un total de \u001b[32m\u001b[1m279\u001b[0m palabras\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "imprimir_resultado(palabras_ed,9)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LyBhzb9GDchv",
        "outputId": "f7baa6aa-c043-4df1-a68b-2429435883cb"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------+--------------+--------------+--------------+--------------+--------------+---------------+--------------+--------------+\n",
            "|    Based     |    Cooked    | Disappointed |    Loved     |    Loved     |   Ordered    |    Ordered    |   Ordered    |  Overpriced  |\n",
            "+--------------+--------------+--------------+--------------+--------------+--------------+---------------+--------------+--------------+\n",
            "|   Stopped    |   Stopped    |    Tasted    |    Tried     |    Waited    |    Waited    | acknowledged  | acknowledged |    added     |\n",
            "+--------------+--------------+--------------+--------------+--------------+--------------+---------------+--------------+--------------+\n",
            "|    added     | anticipated  |   arrived    |   arrived    |   arrived    |    asked     |     asked     |    asked     |    asked     |\n",
            "+--------------+--------------+--------------+--------------+--------------+--------------+---------------+--------------+--------------+\n",
            "|    asked     |   attached   |   avoided    |    boiled    |    burned    |   charged    |    cheated    |   checked    |   checked    |\n",
            "+--------------+--------------+--------------+--------------+--------------+--------------+---------------+--------------+--------------+\n",
            "|   checked    |   claimed    |    closed    | constructed  |  contained   |    cooked    |    cooked     |    cooked    |    cooked    |\n",
            "+--------------+--------------+--------------+--------------+--------------+--------------+---------------+--------------+--------------+\n",
            "|    cooked    |    cooked    |   covered    |   decided    |   decided    |  decorated   |   decorated   |  dedicated   |   desired    |\n",
            "+--------------+--------------+--------------+--------------+--------------+--------------+---------------+--------------+--------------+\n",
            "| disappointed | disappointed | disappointed | disappointed | disappointed | disappointed | disappointed  | disappointed | disappointed |\n",
            "+--------------+--------------+--------------+--------------+--------------+--------------+---------------+--------------+--------------+\n",
            "| disappointed | disappointed | disappointed | disappointed | disappointed | disappointed | disappointed  | disappointed | disappointed |\n",
            "+--------------+--------------+--------------+--------------+--------------+--------------+---------------+--------------+--------------+\n",
            "|  disgusted   | disrespected | disrespected |   dreamed    |   drenched   |   dressed    |     dried     |   dropped    |    dusted    |\n",
            "+--------------+--------------+--------------+--------------+--------------+--------------+---------------+--------------+--------------+\n",
            "|    ended     |    ended     |   enjoyed    |   enjoyed    |   enjoyed    |   enjoyed    |    enjoyed    |    ensued    |   expanded   |\n",
            "+--------------+--------------+--------------+--------------+--------------+--------------+---------------+--------------+--------------+\n",
            "|   expected   |   expected   |   expected   | experienced  |     eyed     |   figured    |   flavored    |   focused    |   focused    |\n",
            "+--------------+--------------+--------------+--------------+--------------+--------------+---------------+--------------+--------------+\n",
            "|    fried     |    fried     |    fried     |    fried     |    fried     |  frustrated  |    greeted    |   greeted    |   grilled    |\n",
            "+--------------+--------------+--------------+--------------+--------------+--------------+---------------+--------------+--------------+\n",
            "|   grilled    |   grossed    |   grossed    |    handed    |   handled    |   happened   |     hated     |    helped    |    helped    |\n",
            "+--------------+--------------+--------------+--------------+--------------+--------------+---------------+--------------+--------------+\n",
            "| highlighted  |    hooked    |  humiliated  |     iced     |   ignored    |   imagined   |   impressed   |  impressed   |  impressed   |\n",
            "+--------------+--------------+--------------+--------------+--------------+--------------+---------------+--------------+--------------+\n",
            "|  impressed   |  impressed   |  impressed   |  impressed   |  impressed   |   included   |   inspired    |   insulted   |   insulted   |\n",
            "+--------------+--------------+--------------+--------------+--------------+--------------+---------------+--------------+--------------+\n",
            "|   insulted   |    lacked    |    lacked    |    lacked    |    liked     |    liked     |     liked     |   limited    |    lined     |\n",
            "+--------------+--------------+--------------+--------------+--------------+--------------+---------------+--------------+--------------+\n",
            "|    listed    |    lived     |   located    |    looked    |    looked    |    loved     |     loved     |    loved     |    loved     |\n",
            "+--------------+--------------+--------------+--------------+--------------+--------------+---------------+--------------+--------------+\n",
            "|    loved     |    loved     |   managed    |    melted    |    missed    |    mixed     |   mortified   |     need     |     need     |\n",
            "+--------------+--------------+--------------+--------------+--------------+--------------+---------------+--------------+--------------+\n",
            "|     need     |    needed    |    needed    |   offered    |    opened    |   opposed    |    ordered    |   ordered    |   ordered    |\n",
            "+--------------+--------------+--------------+--------------+--------------+--------------+---------------+--------------+--------------+\n",
            "|   ordered    |   ordered    |   ordered    |   ordered    |   ordered    |   ordered    |    ordered    |   ordered    |  overcooked  |\n",
            "+--------------+--------------+--------------+--------------+--------------+--------------+---------------+--------------+--------------+\n",
            "|  overcooked  |  overpriced  |  overpriced  |  overpriced  |  overpriced  |  overpriced  |  overwhelmed  | overwhelmed  |    owned     |\n",
            "+--------------+--------------+--------------+--------------+--------------+--------------+---------------+--------------+--------------+\n",
            "|    packed    |    passed    |    passed    |  performed   |   perpared   |  petrified   |    placed     |   pleased    |   pleased    |\n",
            "+--------------+--------------+--------------+--------------+--------------+--------------+---------------+--------------+--------------+\n",
            "|   pleased    |    poured    |   powdered   |   prepared   |    priced    |    priced    |    priced     |  privileged  |  proclaimed  |\n",
            "+--------------+--------------+--------------+--------------+--------------+--------------+---------------+--------------+--------------+\n",
            "|   provided   |    pulled    |    pulled    |    pulled    |    puréed    |  qualified   |     rated     |    rated     |   realized   |\n",
            "+--------------+--------------+--------------+--------------+--------------+--------------+---------------+--------------+--------------+\n",
            "|   realized   |   received   |   received   |   received   | recommended  | recommended  |  recommended  |     red      |     red      |\n",
            "+--------------+--------------+--------------+--------------+--------------+--------------+---------------+--------------+--------------+\n",
            "|  refrained   |   refried    |   refused    |   reheated   |   relaxed    |  relocated   |   reminded    | replenished  |  requested   |\n",
            "+--------------+--------------+--------------+--------------+--------------+--------------+---------------+--------------+--------------+\n",
            "|   returned   |    ripped    |    ripped    |   roasted    |   roasted    |    rolled    |    rushed     |  satisfied   |   screwed    |\n",
            "+--------------+--------------+--------------+--------------+--------------+--------------+---------------+--------------+--------------+\n",
            "|   seasoned   |   seasoned   |    seated    |    seated    |    seated    |    seated    |    seated     |    seemed    |    seemed    |\n",
            "+--------------+--------------+--------------+--------------+--------------+--------------+---------------+--------------+--------------+\n",
            "|    served    |    served    |    served    |    served    |    served    |    served    |    shocked    |    showed    |    sliced    |\n",
            "+--------------+--------------+--------------+--------------+--------------+--------------+---------------+--------------+--------------+\n",
            "|    sliced    |   smeared    |   smelled    |   started    |   started    |    stayed    |    stayed     |   stepped    |   stepped    |\n",
            "+--------------+--------------+--------------+--------------+--------------+--------------+---------------+--------------+--------------+\n",
            "|   stuffed    |   stuffed    |    sucked    |    sucked    |    sucked    |    sucked    |   supposed    |  surprised   |   tailored   |\n",
            "+--------------+--------------+--------------+--------------+--------------+--------------+---------------+--------------+--------------+\n",
            "|    tasted    |    tasted    |    tasted    |    tasted    |    tasted    |    tasted    |    tasted     |   thrilled   |   toasted    |\n",
            "+--------------+--------------+--------------+--------------+--------------+--------------+---------------+--------------+--------------+\n",
            "|   toasted    |   touched    |   tracked    |   treated    |   treated    |   treated    |     tried     |    tried     |    tried     |\n",
            "+--------------+--------------+--------------+--------------+--------------+--------------+---------------+--------------+--------------+\n",
            "|    tried     |    tried     |    tried     |   trimmed    | undercooked  | undercooked  | unexperienced |  uninspired  |  untoasted   |\n",
            "+--------------+--------------+--------------+--------------+--------------+--------------+---------------+--------------+--------------+\n",
            "|  unwrapped   |   uploaded   |     used     |     used     |     used     |   visited    |    vomited    |    voted     |    waited    |\n",
            "+--------------+--------------+--------------+--------------+--------------+--------------+---------------+--------------+--------------+\n",
            "|    waited    |    waited    |    waited    |    waited    |    waited    |    waited    |    waited     |    walked    |    walked    |\n",
            "+--------------+--------------+--------------+--------------+--------------+--------------+---------------+--------------+--------------+\n",
            "|    walked    |    wanted    |    wanted    |    wanted    |    wasted    |   watched    |    watched    |   watched    |   watered    |\n",
            "+--------------+--------------+--------------+--------------+--------------+--------------+---------------+--------------+--------------+\n",
            "|  witnessed   |   wrapped    |              |              |              |              |               |              |              |\n",
            "+--------------+--------------+--------------+--------------+--------------+--------------+---------------+--------------+--------------+\n",
            "Se identificaron un total de \u001b[32m\u001b[1m335\u001b[0m palabras\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Parte 3. Proceso de limpieza.**"
      ],
      "metadata": {
        "id": "70StdqAZa9E9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   **Pregunta 13.**  \n",
        "\n",
        "Ahora realiza un proceso de limpieza del corpus que incluya los siguientes procesos:\n",
        "\n",
        "*   Solo se deben considerar caracteres alfabéticos. Es decir, se eliminan todos los signos de puntuación y caracteres especiales.\n",
        "*   Todos los caracteres alfabéticos se transforman a minúsculas.\n",
        "*   Se deben eliminar todos los espacios en blanco adicionales que se puedan encontrar en cada comentario.\n",
        "\n",
        "Al finalizar dicho proceso de limpieza, imprime el resultado de los primeros 10 comentarios resultantes.\n",
        "   \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xaDUFXHrMvX2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def limpiar_texto(texto):\n",
        "    comentarios_limpios = []\n",
        "    for comentario in texto:\n",
        "        comentario_limpio = re.sub(r'[^a-zA-Z\\s]', '', comentario)  # Solo mantener caracteres alfabéticos (y espacios)\n",
        "        comentario_limpio = comentario_limpio.lower()               # Convertir a minúsculas\n",
        "        comentario_limpio = re.sub(r'\\s+', ' ', comentario_limpio)  # Eliminar espacios en blanco adicionales\n",
        "        comentarios_limpios.append(comentario_limpio)\n",
        "    return comentarios_limpios\n",
        ""
      ],
      "metadata": {
        "id": "K3kQzPOPMx0w"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "comentario_limpio = limpiar_texto(comentarios_sin_saltos)\n",
        "\n",
        "for comentario in comentario_limpio[:10]:\n",
        "    print(comentario)"
      ],
      "metadata": {
        "id": "mYEDlHSFMyJN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2c7f16d-51db-4062-b76f-269a91b67f55"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "wow loved this place\n",
            "crust is not good\n",
            "not tasty and the texture was just nasty\n",
            "stopped by during the late may bank holiday off rick steve recommendation and loved it\n",
            "the selection on the menu was great and so were the prices\n",
            "now i am getting angry and i want my damn pho\n",
            "honeslty it didnt taste that fresh\n",
            "the potatoes were like rubber and you could tell they had been made up ahead of time being kept under a warmer\n",
            "the fries were great too\n",
            "a great touch\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   **Pregunta 14.**  \n",
        "\n",
        "Con el resultado de la limpieza obtenido en la pregunta anterior, realiza ahora un proceso de tokenización por palabras del corpus.\n",
        "\n",
        "Es decir, al final de este proceso de tokenización, debes tener como resultado una lista de listas, donde cada comentario estará tokenizado por palabras.\n",
        "\n",
        "Al terminar calcula el total de tokens obtenido en todo el corpus."
      ],
      "metadata": {
        "id": "WZwEhg2lUSAX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "comentarios_tokenizados = [comentario.split() for comentario in comentario_limpio]\n",
        "total_tokens = sum(len(comentario) for comentario in comentarios_tokenizados)\n",
        "\n",
        "print(f\"Total de tokens: {total_tokens}\")"
      ],
      "metadata": {
        "id": "kbAL9-v0V-jx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c1bca9e-347c-45a4-e678-86bf7fdfb479"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total de tokens: 10777\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   **Pregunta 15.**  \n",
        "\n",
        "Finalmente, en este ejercicio definiremos nuestro conjunto de palabras \"stopwords\", las cuales deberás eliminar de todo el corpus.\n",
        "\n",
        "Recuerda que ejemplos de stopwords son artículos, adverbios, conectivos, etcétera, que tienen frecuencias de aparición muy altas en cualquier documento, pero que no brindan mucho significado en cuanto al significado de un enunciado.\n",
        "\n",
        "Con base a la lista de stopwords que se te proporciona, realiza un proceso de limpieza eliminando todas estas palabras del corpus obtenido en el ejercicio anterior.\n",
        "\n",
        "Obtener cuántos tokens/palabras quedan finalmente en todo el corpus.\n",
        "\n",
        "Obtener cuántos de estos tokens/palabras son diferentes, es decir, cuántos tokens únicos tendrá lo que llamaremos más adelante nuestro vocabulario."
      ],
      "metadata": {
        "id": "EFeu0OJ7WDPD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Considera la siguiente lista como tu conjunto de stopwords:\n",
        "mis_stopwords = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'should', 'now', 'll']"
      ],
      "metadata": {
        "id": "6FP4FF3KXGxm"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "comentarios_sin_stopwords = []\n",
        "\n",
        "for comentario in comentarios_tokenizados:\n",
        "  comentarios_sin_stopwords.append([palabra for palabra in comentario if palabra not in mis_stopwords])\n",
        "\n",
        "total_tokens_restantes = sum(len(comentario) for comentario in comentarios_sin_stopwords)\n",
        "\n",
        "print(f\"Total de tokens restantes: \\033[32m\\033[1m{total_tokens_restantes}\\033[0m\")\n",
        "\n",
        "# Calcular tokens únicos (vocabulario)\n",
        "tokens_unicos = set([palabra for comentario in comentarios_sin_stopwords for palabra in comentario])\n",
        "total_tokens_unicos = len(tokens_unicos)\n",
        "print(f\"Total de tokens únicos (vocabulario): \\033[32m\\033[1m{total_tokens_unicos}\\033[0m\")"
      ],
      "metadata": {
        "id": "CD8yjyq1ZrwY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d23a827-60f2-4608-d858-52aee84582c3"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total de tokens restantes: \u001b[32m\u001b[1m5776\u001b[0m\n",
            "Total de tokens únicos (vocabulario): \u001b[32m\u001b[1m1941\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   **Comentarios**\n",
        "\n",
        "Incluye finalmente tus comentarios de la actividad."
      ],
      "metadata": {
        "id": "NDbKkuxRbLoX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<< incluye aquí tus comentarios >>"
      ],
      "metadata": {
        "id": "o7fzbvqVbUGr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Fin de la Actividad de la semana 2.**"
      ],
      "metadata": {
        "id": "PHaKw_6Ldbaf"
      }
    }
  ]
}