{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**Maestría en Inteligencia Artificial Aplicada**\n",
        "##**Curso: Procesamiento de Lenguaje Natural (NLP)**\n",
        "###Tecnológico de Monterrey\n",
        "###Prof Luis Eduardo Falcón Morales\n",
        "\n",
        "## **Adtividad de la Semana 02**\n",
        "###**Introducción al procesamiento de texto.**"
      ],
      "metadata": {
        "id": "759SG4TyfbUn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En esta actividad deberás utilizar los datos del siguiente archivo que se encuentra en Canvas:\n",
        "\n",
        "MNA_NLP_semana_02_Actividad_datos.txt\n",
        "\n",
        "El archivo contiene comentarios en inglés sobre servicios de comida de la página de Yelp: https://www.yelp.com/ .\n",
        "\n",
        "Son mil comentarios y forman parte del conjunto de datos que se encuentra en el Machine Learning Repository de la UCI, llamado \"Sentiment Labelled Sentences\": https://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences#\n"
      ],
      "metadata": {
        "id": "6ue1YAKx3XDo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Parte 1. Cargamos los datos.**   "
      ],
      "metadata": {
        "id": "Zj-h4drXD-X9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cargar los datos del archivo indicado y obtener una lista de longitud de 1000 strings/comentarios.\n",
        "\n",
        "Por el momento solamente requerimos las bibliotecas de Numpy y re, para el manejo de los arreglos y de las expresiones regulares en Python.\n",
        "\n",
        "En particular, no necesitarás en esta actividad la biblioteca de Pandas.\n",
        "\n",
        "###**NOTA: En esta actividad no debes importar nada más, con estas dos bibliotecas será *suficiente*.**"
      ],
      "metadata": {
        "id": "BY6yifxscfrx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np       # importamos Numpy para el manejo de los arreglos.\n",
        "import re                # importamos re para el manejo de las expresiones regulares.\n",
        "import gdown             # importamos gdown para descargar el archivo de trabajo\n",
        "import os                # importamos os para manejo de archivos y directorios\n",
        "import tabulate as tab   # importamos tabulate para impresión de tablas"
      ],
      "metadata": {
        "id": "OJ26dAfhdFnf"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs('datos', exist_ok=True)\n",
        "datos_url = 'https://drive.google.com/uc?id=1qVQ5sn901fDCQMRP_NpaHEoTEEOlrfM3'\n",
        "output = 'datos/actividad_datos.txt'\n",
        "\n",
        "print(\"Descargando datos\")\n",
        "gdown.download(datos_url, output, quiet=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wqc0R1HMRP-x",
        "outputId": "e81b418e-93bf-42c0-a026-5b7fd5b37651"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Descargando datos\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1qVQ5sn901fDCQMRP_NpaHEoTEEOlrfM3\n",
            "To: /home/jarcos/datos/actividad_datos.txt\n",
            "100%|█████████████████████████████████████████████████████████████████████| 59.9k/59.9k [00:00<00:00, 508kB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'datos/actividad_datos.txt'"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ejecuta las siguientes instrucciones para cargar la información del achivo dado:\n",
        "\n",
        "with open('datos/actividad_datos.txt',        # puedes actualizar la ruta a tu archivo, en dado caso.\n",
        "          mode='r',                           # abrimos el archivo en modo lectura.\n",
        "          ) as f:\n",
        "    docs = f.readlines()                      # separamos cada comentario por líneas\n",
        "\n",
        "f.close()                         # ya que tenemos la información en la variable docs, cerramos el archivo"
      ],
      "metadata": {
        "id": "QHUmJyjDdGNP"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(docs) == list   # Verifica que tu variable \"docs\" es una lista"
      ],
      "metadata": {
        "id": "L6WzrSrodG-Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7eabafb-e25a-41d8-a52e-cdd367cd8c66"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(docs)==1000  # verifica que la longitud de \"docs\" es de mil comentarios."
      ],
      "metadata": {
        "id": "QIK1u9WS2FtS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95d194ee-594e-4eb1-e81a-792df4cf6185"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "docs[0:10]     # observa algunos de los primeros comentarios"
      ],
      "metadata": {
        "id": "9AMLIfQvJqNZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6dc302ce-4391-4152-82a9-21f5b6e40c68"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Wow... Loved this place.\\n',\n",
              " 'Crust is not good.\\n',\n",
              " 'Not tasty and the texture was just nasty.\\n',\n",
              " 'Stopped by during the late May bank holiday off Rick Steve recommendation and loved it.\\n',\n",
              " 'The selection on the menu was great and so were the prices.\\n',\n",
              " 'Now I am getting angry and I want my damn pho.\\n',\n",
              " \"Honeslty it didn't taste THAT fresh.)\\n\",\n",
              " 'The potatoes were like rubber and you could tell they had been made up ahead of time being kept under a warmer.\\n',\n",
              " 'The fries were great too.\\n',\n",
              " 'A great touch.\\n']"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Parte 2: sección de preguntas (regex).**   \n"
      ],
      "metadata": {
        "id": "k_ewoagic5jc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Instrucciones:**\n",
        "\n",
        "###**A continuación deberás contestar cada una de las preguntas que te piden usando expresiones regulares (regex).**\n",
        "\n",
        "###**Por el momento no hay restricción en cuanto al número de líneas de código que agregues, pero trata de incluir las mínimas posibles.**"
      ],
      "metadata": {
        "id": "X-eMJa3DFCIV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   **Pregunta 1.**\n",
        "\n",
        "Busca y elimina todos los saltos de línea '\\n' que se encuentran al final de cada comentario.\n",
        "\n",
        "Una vez finalizado, imprime los primeros 10 comentarios del resultado obtenido.\n"
      ],
      "metadata": {
        "id": "78nJMemzn5a5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "comentarios_sin_saltos = [re.sub(r'\\n$', '', registro) for registro in docs]\n",
        "for comentario in comentarios_sin_saltos[:10]:\n",
        "    print(comentario)"
      ],
      "metadata": {
        "id": "PwbYYIuZn8pE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "200dd6d9-8b02-4689-ac43-2ce59844a76d"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wow... Loved this place.\n",
            "Crust is not good.\n",
            "Not tasty and the texture was just nasty.\n",
            "Stopped by during the late May bank holiday off Rick Steve recommendation and loved it.\n",
            "The selection on the menu was great and so were the prices.\n",
            "Now I am getting angry and I want my damn pho.\n",
            "Honeslty it didn't taste THAT fresh.)\n",
            "The potatoes were like rubber and you could tell they had been made up ahead of time being kept under a warmer.\n",
            "The fries were great too.\n",
            "A great touch.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   **Pregunta 2.**  \n",
        "\n",
        "Busca e imprime todas las palabras que terminan con dos o más signos de admiración seguidos, por ejemplo \"!!!\".\n",
        "\n",
        "Debes imprimir tanto la palabra como la totalidad de signos de admiración que le siguen.\n",
        "\n",
        "Indica cuántos resultados obtuviste.\n",
        "\n"
      ],
      "metadata": {
        "id": "VWeKQC93ctEo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def imprimir_resultado(palabras,columnas):\n",
        "  filas = []\n",
        "  ordenados = sorted(palabras)\n",
        "  for i in range(0, len(ordenados), columnas):\n",
        "        fila = ordenados[i:i + columnas]\n",
        "\n",
        "        while len(fila) < columnas:\n",
        "            fila.append(\"\")\n",
        "        filas.append(fila)\n",
        "\n",
        "  print(tab.tabulate(filas, tablefmt=\"grid\", stralign=\"center\"))\n",
        "  print(f\"Se identificaron un total de \\033[32m\\033[1m{len(palabras)}\\033[0m palabras\")"
      ],
      "metadata": {
        "id": "ixtW-jri6upD"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "palabras_con_exclamaciones = []\n",
        "for comentario in comentarios_sin_saltos:\n",
        "    palabras_con_exclamaciones.extend(re.findall(r'\\b\\w+!{2,}', comentario))\n",
        "\n",
        "imprimir_resultado(palabras_con_exclamaciones,3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LwO2Gz0Y7vs1",
        "outputId": "eee42f8c-47b9-43e6-f72f-6f2d9d0f36e6"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------+----------------------------+---------------------+\n",
            "| APPETIZERS!!!  |        DELICIOUS!!         |     FLY!!!!!!!!     |\n",
            "+----------------+----------------------------+---------------------+\n",
            "| Firehouse!!!!! |            Up!!            |      amazing!!      |\n",
            "+----------------+----------------------------+---------------------+\n",
            "|   amazing!!!   | amazing!!!!!!!!!!!!!!!!!!! |      awesome!!      |\n",
            "+----------------+----------------------------+---------------------+\n",
            "|  biscuits!!!   |         buffet!!!          |    delicious!!!     |\n",
            "+----------------+----------------------------+---------------------+\n",
            "| disappointed!! |      disappointing!!!      |        dry!!        |\n",
            "+----------------+----------------------------+---------------------+\n",
            "|     far!!      |           good!!           | great!!!!!!!!!!!!!! |\n",
            "+----------------+----------------------------+---------------------+\n",
            "|    here!!!     |           it!!!!           |     otherwise!!     |\n",
            "+----------------+----------------------------+---------------------+\n",
            "|    packed!!    |    shawarrrrrrma!!!!!!     |     steak!!!!!      |\n",
            "+----------------+----------------------------+---------------------+\n",
            "|    style!!     |          yucky!!!          |                     |\n",
            "+----------------+----------------------------+---------------------+\n",
            "Se identificaron un total de \u001b[32m\u001b[1m26\u001b[0m palabras\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   **Pregunta 3.**  \n",
        "\n",
        "Busca e imprime todas las palabras que están escritas totalmente en mayúsculas. Cada coincidencia debe ser una sola palabra.\n",
        "\n",
        "Indica cuántas palabras encontraste.\n",
        "\n"
      ],
      "metadata": {
        "id": "-s3okBqL96TT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "set_mayusculas = set()\n",
        "for comentario in comentarios_sin_saltos:\n",
        "    set_mayusculas.update(re.findall(r'\\b[A-Z]+\\b', comentario))\n",
        "\n",
        "palabras_mayusculas = list(set_mayusculas)\n",
        "\n",
        "imprimir_resultado(palabras_mayusculas,6)"
      ],
      "metadata": {
        "id": "yKHJkZKo_nW5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0134ad9a-fbac-416e-bfd5-507c9c840260"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+---------------+---------------+------------+-----------+------------+\n",
            "|     A     |     AGAIN     |      ALL      |     AN     |    AND    | APPETIZERS |\n",
            "+-----------+---------------+---------------+------------+-----------+------------+\n",
            "|   AVOID   |     AYCE      |      AZ       |    BACK    |   BARE    |  BARGAIN   |\n",
            "+-----------+---------------+---------------+------------+-----------+------------+\n",
            "|    BBQ    |     BEST      |    BETTER     |  BITCHES   |   BLAND   | CONCLUSION |\n",
            "+-----------+---------------+---------------+------------+-----------+------------+\n",
            "| DELICIOUS | ESTABLISHMENT |     EVER      | EXPERIENCE | FANTASTIC |   FLAVOR   |\n",
            "+-----------+---------------+---------------+------------+-----------+------------+\n",
            "|    FLY    |    FORWARD    |   FREEZING    |     FS     |    GC     |     GO     |\n",
            "+-----------+---------------+---------------+------------+-----------+------------+\n",
            "|   GREAT   |      HAD      |     HANDS     |  HAPPENED  |   HAVE    |    HOUR    |\n",
            "+-----------+---------------+---------------+------------+-----------+------------+\n",
            "|     I     |      IN       | INCONSIDERATE |     IT     |   LEGIT   |   LOVED    |\n",
            "+-----------+---------------+---------------+------------+-----------+------------+\n",
            "|     M     |  MANAGEMENT   |     MANY      |    MGM     |   MUST    |   NASTY    |\n",
            "+-----------+---------------+---------------+------------+-----------+------------+\n",
            "|   NEVER   |      NO       |     NONE      |    NOT     |    NOW    |    NYC     |\n",
            "+-----------+---------------+---------------+------------+-----------+------------+\n",
            "|    OF     |      OK       |      OMG      | OVERPRICED |  OWNERS   |   PEOPLE   |\n",
            "+-----------+---------------+---------------+------------+-----------+------------+\n",
            "|  PERFECT  |     REAL      |    REALLY     |     RI     |   RUDE    |  SCREAMS   |\n",
            "+-----------+---------------+---------------+------------+-----------+------------+\n",
            "|  SHOULD   |     STALE     |     STEP      |     T      |   THAT    |    THE     |\n",
            "+-----------+---------------+---------------+------------+-----------+------------+\n",
            "|   THIS    |     TIME      |     TOLD      |   TOTAL    |    TV     |   UNREAL   |\n",
            "+-----------+---------------+---------------+------------+-----------+------------+\n",
            "|   VERY    |     WASTE     |      WAY      |    WEAK    |   WHAT    |    WILL    |\n",
            "+-----------+---------------+---------------+------------+-----------+------------+\n",
            "|   WORST   |               |               |            |           |            |\n",
            "+-----------+---------------+---------------+------------+-----------+------------+\n",
            "Se identificaron un total de \u001b[32m\u001b[1m85\u001b[0m palabras\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   **Pregunta 4.**  \n",
        "\n",
        "Busca e imprime los comentarios en donde todos los caracteres alfabéticos (letras) están en mayúsculas.\n",
        "\n",
        "Cada coincidencia encontrada debe ser todo el comentario/enunciado.\n",
        "\n",
        "Indica cuántos resultados obtuviste.\n"
      ],
      "metadata": {
        "id": "GX8eYyDoMZma"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "comentarios_mayusculas = []\n",
        "\n",
        "for comentario in comentarios_sin_saltos:\n",
        "    if re.fullmatch(r'[A-Z\\s\\W]+', comentario):\n",
        "        comentarios_mayusculas.append(comentario)\n",
        "\n",
        "imprimir_resultado(comentarios_mayusculas,1)"
      ],
      "metadata": {
        "id": "K8VuZxvTMYj6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af82a63a-c483-4a2c-d6f4-671eb689f3df"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------------------------------------------------------------+\n",
            "|                      AVOID THIS ESTABLISHMENT!                       |\n",
            "+----------------------------------------------------------------------+\n",
            "|                             DELICIOUS!!                              |\n",
            "+----------------------------------------------------------------------+\n",
            "|                   RUDE & INCONSIDERATE MANAGEMENT.                   |\n",
            "+----------------------------------------------------------------------+\n",
            "|                         TOTAL WASTE OF TIME.                         |\n",
            "+----------------------------------------------------------------------+\n",
            "| WILL NEVER EVER GO BACK AND HAVE TOLD MANY PEOPLE WHAT HAD HAPPENED. |\n",
            "+----------------------------------------------------------------------+\n",
            "Se identificaron un total de \u001b[32m\u001b[1m5\u001b[0m palabras\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   **Pregunta 5.**  \n",
        "\n",
        "Busca e imprime todas las palabras que tengan una vocal acentuada, del tipo á, é, í, ó, ú.\n",
        "\n",
        "Indica cuántos resultados obtuviste."
      ],
      "metadata": {
        "id": "a1i6qv7-McmU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "palabras_tilde = []\n",
        "\n",
        "for comentario in comentarios_sin_saltos:\n",
        "    palabras_tilde.extend(re.findall(r'\\b\\w*[áéíóú]\\w*\\b', comentario, re.IGNORECASE))\n",
        "\n",
        "imprimir_resultado(palabras_tilde,3)"
      ],
      "metadata": {
        "id": "nZZ5zKUOMeGD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9831de5d-c143-4794-a5a2-5e813cc8f8cc"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+--------+--------+\n",
            "| Café | fiancé | puréed |\n",
            "+------+--------+--------+\n",
            "Se identificaron un total de \u001b[32m\u001b[1m3\u001b[0m palabras\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   **Pregunta 6.**  \n",
        "\n",
        "Busca e imprime todas las cantidades numéricas monetarias, enteras o con decimales, que inician con el símbolo $\\$$.\n",
        "\n",
        "Indica cuántos resultados obtuviste."
      ],
      "metadata": {
        "id": "ZmPiAI82Mfb3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cantidades_monetarias = []\n",
        "\n",
        "for comentario in comentarios_sin_saltos:\n",
        "    cantidades_monetarias.extend(re.findall(r'\\$\\s*(\\d+(?:\\.\\d+)?)', comentario))\n",
        "\n",
        "imprimir_resultado(cantidades_monetarias,4)"
      ],
      "metadata": {
        "id": "6vhe9-Y-MhL9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8594936-3ba1-4a94-e11b-674ca8b348a4"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----+----+-------+\n",
            "| 11.99 | 12 | 17 | 20    |\n",
            "+-------+----+----+-------+\n",
            "|  3    | 35 |  4 |  7.85 |\n",
            "+-------+----+----+-------+\n",
            "Se identificaron un total de \u001b[32m\u001b[1m8\u001b[0m palabras\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   **Pregunta 7.**  \n",
        "\n",
        "Busca e imprime todas las palabras que sean variantes de la palabra \"love\", sin importar si incluyen mayúsculas o minúsculas, o la manera en que esté conjugada o alguna otra variación que se haga con dicha palabra.\n",
        "\n",
        "Indica cuántos resultados obtuviste."
      ],
      "metadata": {
        "id": "2j-HpvhwMhq2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "regex = re.compile(r'\\b[Ll]ov(?:e[sd]?|ing|er|ers|ely|able|es|ed|ing|ingly|esque|ish|less|ness|ful|ment|s|d)\\b', re.IGNORECASE)\n",
        "palabras_love = []\n",
        "\n",
        "for comentario in comentarios_sin_saltos:\n",
        "  palabras_love.extend(regex.findall(comentario))\n",
        "\n",
        "imprimir_resultado(palabras_love,6)"
      ],
      "metadata": {
        "id": "kqqyRChVMjol",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc84c7cd-4d86-48c2-f767-690341a90789"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+-------+--------+--------+-------+--------+\n",
            "| LOVED  | LOVED |  Love  |  Love  | Loved | Loved  |\n",
            "+--------+-------+--------+--------+-------+--------+\n",
            "|  love  | love  |  love  |  love  | love  |  love  |\n",
            "+--------+-------+--------+--------+-------+--------+\n",
            "|  love  | love  |  love  |  love  | love  |  love  |\n",
            "+--------+-------+--------+--------+-------+--------+\n",
            "|  love  | love  |  love  |  love  | love  | loved  |\n",
            "+--------+-------+--------+--------+-------+--------+\n",
            "| loved  | loved | loved  | loved  | loved | lovely |\n",
            "+--------+-------+--------+--------+-------+--------+\n",
            "| lovely | lover | lovers | lovers | loves | loving |\n",
            "+--------+-------+--------+--------+-------+--------+\n",
            "Se identificaron un total de \u001b[32m\u001b[1m36\u001b[0m palabras\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   **Pregunta 8.**  \n",
        "\n",
        "Busca e imprime todas las palabras, variantes de \"so\" y \"good\", que tengan dos o más \"o\" en \"so\" y 3 o más \"o\" en good.\n",
        "\n",
        "Indica cuántas encontraste.\n"
      ],
      "metadata": {
        "id": "Ctb-NTY3MkYG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "regex_so = re.compile(r'\\bso+o+\\b', re.IGNORECASE)\n",
        "regex_good = re.compile(r'\\bgo+o+od\\w*\\b', re.IGNORECASE)\n",
        "\n",
        "# Buscar y almacenar todas las variantes de \"so\" y \"good\"\n",
        "palabras_so = []\n",
        "palabras_good = []\n",
        "for comentario in comentarios_sin_saltos:\n",
        "    palabras_so.extend(regex_so.findall(comentario))\n",
        "    palabras_good.extend(regex_good.findall(comentario))\n",
        "\n",
        "imprimir_resultado(palabras_so,3)\n",
        "imprimir_resultado(palabras_good,1)"
      ],
      "metadata": {
        "id": "A8Nf3B_cMlqg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de51a4b2-1303-4680-828e-5ca656dec308"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-------+-------+\n",
            "| Sooooo  | soooo | soooo |\n",
            "+---------+-------+-------+\n",
            "| soooooo |       |       |\n",
            "+---------+-------+-------+\n",
            "Se identificaron un total de \u001b[32m\u001b[1m4\u001b[0m palabras\n",
            "+--------+\n",
            "| gooodd |\n",
            "+--------+\n",
            "Se identificaron un total de \u001b[32m\u001b[1m1\u001b[0m palabras\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   **Pregunta 9.**  \n",
        "\n",
        "Busca e imprime todas las palabras que tengan una longitud mayor estrictamente a 10 caracteres alfabéticos.\n",
        "\n",
        "No se consideran los signos de puntuación o caracteres especiales en la longitud de estas cadenas, solo caracteres alfabéticos en mayúsculas o minúsculas.\n",
        "\n",
        "Indica la cantidad de palabras encontradas.\n"
      ],
      "metadata": {
        "id": "hkak1opjMmlk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "regex = re.compile(r'\\b[a-zA-Z]{11,}\\b')\n",
        "palabras_largas = []\n",
        "for comentario in comentarios_sin_saltos:\n",
        "  palabras_largas.extend(regex.findall(comentario))\n",
        "\n",
        "imprimir_resultado(palabras_largas,7)"
      ],
      "metadata": {
        "id": "PYxdp3uhMoD0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e50a3c21-0c1b-4065-fba7-2a9a36b1a93d",
        "collapsed": true
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+-----------------+----------------+----------------+----------------+----------------+-----------------+\n",
            "|   Disappointed    |  Disappointing  | ESTABLISHMENT  |  Furthermore   | INCONSIDERATE  |  Interesting   |  Mediterranean  |\n",
            "+-------------------+-----------------+----------------+----------------+----------------+----------------+-----------------+\n",
            "|    Outstanding    |  Philadelphia   |  Smashburger   | Unfortunately  | Unfortunately  | Unfortunately  |   Veggitarian   |\n",
            "+-------------------+-----------------+----------------+----------------+----------------+----------------+-----------------+\n",
            "| WAAAAAAyyyyyyyyyy | Wienerschnitzel | accommodations |  accordingly   |  acknowledged  |  acknowledged  |   anticipated   |\n",
            "+-------------------+-----------------+----------------+----------------+----------------+----------------+-----------------+\n",
            "|    beautifully    |   calligraphy   |  caterpillar   |  cheeseburger  |  cheeseburger  |  cheesecurds   |  circumstances  |\n",
            "+-------------------+-----------------+----------------+----------------+----------------+----------------+-----------------+\n",
            "|    combination    |   combination   |  comfortable   |  comfortable   |  compliments   |  connoisseur   |   considering   |\n",
            "+-------------------+-----------------+----------------+----------------+----------------+----------------+-----------------+\n",
            "|    considering    |   considering   |  considering   |  constructed   |  corporation   |  deliciously   |  descriptions   |\n",
            "+-------------------+-----------------+----------------+----------------+----------------+----------------+-----------------+\n",
            "|   deuchebaggery   |  disappointed   |  disappointed  |  disappointed  |  disappointed  |  disappointed  |  disappointed   |\n",
            "+-------------------+-----------------+----------------+----------------+----------------+----------------+-----------------+\n",
            "|   disappointed    |  disappointed   |  disappointed  |  disappointed  |  disappointed  |  disappointed  |  disappointed   |\n",
            "+-------------------+-----------------+----------------+----------------+----------------+----------------+-----------------+\n",
            "|   disappointed    |  disappointed   |  disappointed  |  disappointed  |  disappointed  | disappointing  |  disappointing  |\n",
            "+-------------------+-----------------+----------------+----------------+----------------+----------------+-----------------+\n",
            "|   disappointing   |  disappointing  | disappointment | disappointment | disappointment | disappointment | disapppointment |\n",
            "+-------------------+-----------------+----------------+----------------+----------------+----------------+-----------------+\n",
            "|    disgraceful    |  disrespected   |  disrespected  |  drastically   |  enthusiastic  | establishment  |  establishment  |\n",
            "+-------------------+-----------------+----------------+----------------+----------------+----------------+-----------------+\n",
            "|   establishment   |   exceptional   |  expectations  |  experienced   |  experiencing  | extraordinary  |   grandmother   |\n",
            "+-------------------+-----------------+----------------+----------------+----------------+----------------+-----------------+\n",
            "|    highlighted    |   hospitality   |  imagination   |  imaginative   |  immediately   |  immediately   |   immediately   |\n",
            "+-------------------+-----------------+----------------+----------------+----------------+----------------+-----------------+\n",
            "|    inexpensive    |   inexpensive   |  informative   |  ingredients   |  interesting   |  interesting   |   maintaining   |\n",
            "+-------------------+-----------------+----------------+----------------+----------------+----------------+-----------------+\n",
            "|   neighborhood    |  neighborhood   |  opportunity   |  opportunity   |  outrageously  |  outstanding   |   outstanding   |\n",
            "+-------------------+-----------------+----------------+----------------+----------------+----------------+-----------------+\n",
            "|    overwhelmed    |   overwhelmed   |  presentation  |  presentation  |  professional  |  professional  |   profiterole   |\n",
            "+-------------------+-----------------+----------------+----------------+----------------+----------------+-----------------+\n",
            "|  recommendation   | recommendation  | recommendation |  recommended   |  recommended   |  recommended   |  recommending   |\n",
            "+-------------------+-----------------+----------------+----------------+----------------+----------------+-----------------+\n",
            "|   relationship    |   replenished   |  reservation   |  restaurants   |  restaurants   |  restaurants   |  shawarrrrrrma  |\n",
            "+-------------------+-----------------+----------------+----------------+----------------+----------------+-----------------+\n",
            "|    suggestions    |   traditional   |  transcendant  |  unbelievable  |  unbelievably  |  undercooked   |   undercooked   |\n",
            "+-------------------+-----------------+----------------+----------------+----------------+----------------+-----------------+\n",
            "|   underwhelming   |  underwhelming  | unexperienced  | unfortunately  | unprofessional |  unsatisfying  |   ventilation   |\n",
            "+-------------------+-----------------+----------------+----------------+----------------+----------------+-----------------+\n",
            "|    vinaigrette    |                 |                |                |                |                |                 |\n",
            "+-------------------+-----------------+----------------+----------------+----------------+----------------+-----------------+\n",
            "Se identificaron un total de \u001b[32m\u001b[1m141\u001b[0m palabras\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   **Pregunta 10.**  \n",
        "\n",
        "Busca e imprime todas las palabras que inician con una letra mayúscula y terminan con una minúscula, pero que además no sea la primera palabra del comentario/string.\n",
        "\n",
        "Indica la cantidad de resultados obtenidos."
      ],
      "metadata": {
        "id": "ApjTNzSxMpDc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "patron_10 = r\"(?<!^)\\b[A-Z][a-z]*[a-z]\\b\"\n",
        "palabras_10 = []\n",
        "for comentario in comentarios_sin_saltos:\n",
        "  palabras_10.extend(re.findall(patron_10, comentario))\n",
        "\n",
        "imprimir_resultado(palabras_10,7)"
      ],
      "metadata": {
        "id": "Vb0ndRGAMqdL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74952822-683d-45c0-8d2d-3294fc3c8cea"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+-------------+-----------+-----------------+---------------+---------------+--------------+\n",
            "|    After    |    After    |   After   |   Albondigas    |      All      |      All      |     All      |\n",
            "+-------------+-------------+-----------+-----------------+---------------+---------------+--------------+\n",
            "|     All     |    Also     |   Also    |      Also       |     Also      |     Also      |   Although   |\n",
            "+-------------+-------------+-----------+-----------------+---------------+---------------+--------------+\n",
            "|     And     |     And     |  Anyway   |     Anyway      |    Anyways    |      Are      |     Area     |\n",
            "+-------------+-------------+-----------+-----------------+---------------+---------------+--------------+\n",
            "|    Aria     |     As      |    As     |       As        |      As       |      At       |      At      |\n",
            "+-------------+-------------+-----------+-----------------+---------------+---------------+--------------+\n",
            "|   Attack    |    Baba     |   Bachi   |      Bachi      |     Back      |      Bad      |     Bad      |\n",
            "+-------------+-------------+-----------+-----------------+---------------+---------------+--------------+\n",
            "|     Bar     |  Baseball   |    Bay    |       Bay       |      Bay      |      Bay      |      Be      |\n",
            "+-------------+-------------+-----------+-----------------+---------------+---------------+--------------+\n",
            "|    Bean     |  Bellagio   |   Best    |      Best       |     Best      |      Big      |    Bisque    |\n",
            "+-------------+-------------+-----------+-----------------+---------------+---------------+--------------+\n",
            "|   Bisque    |    Blah     |  Bloody   |      Both       |     Both      |    Bouchon    |    Breeze    |\n",
            "+-------------+-------------+-----------+-----------------+---------------+---------------+--------------+\n",
            "|  Brushfire  |   Buffet    |  Buffet   |    Buldogis     |     Bunch     |    Burger     |  Burrittos   |\n",
            "+-------------+-------------+-----------+-----------------+---------------+---------------+--------------+\n",
            "|   Bussell   |     By      | Caballero |     Caesar      |   Camelback   |     Cape      |    Carly     |\n",
            "+-------------+-------------+-----------+-----------------+---------------+---------------+--------------+\n",
            "|   Cartel    |   Casino    |  Chicken  |     Chicken     |    Chinese    |    Chinese    |   Chipotle   |\n",
            "+-------------+-------------+-----------+-----------------+---------------+---------------+--------------+\n",
            "|  Christmas  |    Cibo     |  Classy   |       Cod       |    Coffee     |     Come      |   Company    |\n",
            "+-------------+-------------+-----------+-----------------+---------------+---------------+--------------+\n",
            "|   Costco    |    Cotta    |   Crema   |    Crystals     |   Customer    |     Cute      |  Definitely  |\n",
            "+-------------+-------------+-----------+-----------------+---------------+---------------+--------------+\n",
            "|  Delicious  |   Delight   |   Denny   |     Despite     | Disappointed  |      Dog      |     Dos      |\n",
            "+-------------+-------------+-----------+-----------------+---------------+---------------+--------------+\n",
            "|   Drinks    |    Dylan    | Edinburgh |       Egg       |   Eggplant    |      Elk      |   English    |\n",
            "+-------------+-------------+-----------+-----------------+---------------+---------------+--------------+\n",
            "|     Eve     |    Every    | Everyone  |   Everything    |   Excalibur   |  Experience   |    Filet     |\n",
            "+-------------+-------------+-----------+-----------------+---------------+---------------+--------------+\n",
            "|    Filet    |  Firehouse  |  Flower   |     Flower      |     Food      |     Food      |     For      |\n",
            "+-------------+-------------+-----------+-----------------+---------------+---------------+--------------+\n",
            "|     For     |     For     |    For    |    Francisco    |   Frenchman   |    Fridays    |    Friend    |\n",
            "+-------------+-------------+-----------+-----------------+---------------+---------------+--------------+\n",
            "|   Frozen    | Furthermore |  Ganoush  |      Give       |     Gold      |     Good      |     Good     |\n",
            "+-------------+-------------+-----------+-----------------+---------------+---------------+--------------+\n",
            "|    Good     |   Gordon    |  Gourmet  |      Great      |     Great     |     Great     |    Great     |\n",
            "+-------------+-------------+-----------+-----------------+---------------+---------------+--------------+\n",
            "|    Great    |    Great    |   Great   |      Great      |     Greek     |     Greek     |    Greek     |\n",
            "+-------------+-------------+-----------+-----------------+---------------+---------------+--------------+\n",
            "|    Green    |    Grill    |  Gringos  |      Gyros      |      Ha       |    Halibut    |     Han      |\n",
            "+-------------+-------------+-----------+-----------------+---------------+---------------+--------------+\n",
            "|    Hard     |  Hawaiian   |    He     |       He        |     Heart     |    Heimer     |     Hiro     |\n",
            "+-------------+-------------+-----------+-----------------+---------------+---------------+--------------+\n",
            "|  Honestly   |    Host     |    Hot    |       Hot       |      Hot      |      How      |   However    |\n",
            "+-------------+-------------+-----------+-----------------+---------------+---------------+--------------+\n",
            "|   However   |    Hunan    |    Hut    |      Ians       |      If       |      If       |      If      |\n",
            "+-------------+-------------+-----------+-----------------+---------------+---------------+--------------+\n",
            "|     If      |     If      |    If     |       If        |      If       |      In       |      In      |\n",
            "+-------------+-------------+-----------+-----------------+---------------+---------------+--------------+\n",
            "|     In      |     In      |    In     |     Indian      |    Indian     |    Insults    |   Ironman    |\n",
            "+-------------+-------------+-----------+-----------------+---------------+---------------+--------------+\n",
            "|     It      |     It      |    It     |       It        |      It       |      It       |      It      |\n",
            "+-------------+-------------+-----------+-----------------+---------------+---------------+--------------+\n",
            "|     It      |     It      |    It     |     Italian     |    Italian    |   Jamaican    |   Japanese   |\n",
            "+-------------+-------------+-----------+-----------------+---------------+---------------+--------------+\n",
            "|    Jeff     |    Jenni    |   Joey    |     Kabuki      |     Khao      |     Large     |     Las      |\n",
            "+-------------+-------------+-----------+-----------------+---------------+---------------+--------------+\n",
            "|   Lastly    |    Lemon    |   Level   |     Lobster     |    Lobster    |    Lobster    |     Long     |\n",
            "+-------------+-------------+-----------+-----------------+---------------+---------------+--------------+\n",
            "|    Lordy    |    Love     |   Love    |      Loved      |     Loved     |      Lox      |     Luke     |\n",
            "+-------------+-------------+-----------+-----------------+---------------+---------------+--------------+\n",
            "|  Macarons   |   Madison   | Magazine  |      Magic      |     Main      |     Maine     |   Mandalay   |\n",
            "+-------------+-------------+-----------+-----------------+---------------+---------------+--------------+\n",
            "|    Mango    |    Maria    |   Mary    |       May       |     Maybe     | Mediterranean |    Mellow    |\n",
            "+-------------+-------------+-----------+-----------------+---------------+---------------+--------------+\n",
            "|   Mexican   |   Mexican   |  Mirage   |      Mmmm       |      Mom      |   Mushroom    |      My      |\n",
            "+-------------+-------------+-----------+-----------------+---------------+---------------+--------------+\n",
            "|     My      |     My      |    My     |       Nan       |   Needless    |   Needless    |    Never     |\n",
            "+-------------+-------------+-----------+-----------------+---------------+---------------+--------------+\n",
            "|    Nice     |    Nice     |   Ninja   |       No        |      No       |     Nobu      |     Noca     |\n",
            "+-------------+-------------+-----------+-----------------+---------------+---------------+--------------+\n",
            "|    North    |     Not     |    Not    |       Not       |      Not      |      Not      |     Now      |\n",
            "+-------------+-------------+-----------+-----------------+---------------+---------------+--------------+\n",
            "|     Of      |     Oh      |    On     |       On        |      On       |      On       |      On      |\n",
            "+-------------+-------------+-----------+-----------------+---------------+---------------+--------------+\n",
            "|     One     |   Ordered   |   Otto    |       Our       |      Our      |      Our      |     Our      |\n",
            "+-------------+-------------+-----------+-----------------+---------------+---------------+--------------+\n",
            "| Outstanding |   Overall   |  Overall  |     Overall     |    Overall    |    Overall    |     Palm     |\n",
            "+-------------+-------------+-----------+-----------------+---------------+---------------+--------------+\n",
            "|    Panna    |  Paradise   |  Perfect  |     Perfect     |    Perhaps    |  Phenomenal   | Philadelphia |\n",
            "+-------------+-------------+-----------+-----------------+---------------+---------------+--------------+\n",
            "|     Pho     |     Pho     |    Pho    |       Pho       |    Phoenix    |    Phoenix    |   Phoenix    |\n",
            "+-------------+-------------+-----------+-----------------+---------------+---------------+--------------+\n",
            "|   Phoenix   |  Pineapple  |   Pita    |      Pizza      |     Pizza     |     Place     |    Plater    |\n",
            "+-------------+-------------+-----------+-----------------+---------------+---------------+--------------+\n",
            "|    Plus     |    Point    |   Poor    |     Prices      |    Prices     |   Probably    |     Pros     |\n",
            "+-------------+-------------+-----------+-----------------+---------------+---------------+--------------+\n",
            "|   Ramsey    |   Really    |   Rice    |      Rick       |    Risotto    |     Rock      |     Roll     |\n",
            "+-------------+-------------+-----------+-----------------+---------------+---------------+--------------+\n",
            "|    Sadly    |    Salad    |  Salads   |      Same       |      San      |      Sat      |  Scottsdale  |\n",
            "+-------------+-------------+-----------+-----------------+---------------+---------------+--------------+\n",
            "|    Seat     |  Seriously  |  Service  |     Service     |    Service    |     Shop      |  Similarly   |\n",
            "+-------------+-------------+-----------+-----------------+---------------+---------------+--------------+\n",
            "| Smashburger |     Soi     |   Sorry   |      Soups      |     Sour      |    Sprouts    |   Standard   |\n",
            "+-------------+-------------+-----------+-----------------+---------------+---------------+--------------+\n",
            "|    Stars    |    Steak    |   Steak   |    Steiners     |     Steve     |    Stopped    |    Strike    |\n",
            "+-------------+-------------+-----------+-----------------+---------------+---------------+--------------+\n",
            "|    Strip    |    Strip    |  Subway   |     Subway      |    Subway     |      Sun      |    Sunday    |\n",
            "+-------------+-------------+-----------+-----------------+---------------+---------------+--------------+\n",
            "|    Sushi    |    Taco     |   Tasty   |      Thai       |     Thai      |     Thai      |     Thai     |\n",
            "+-------------+-------------+-----------+-----------------+---------------+---------------+--------------+\n",
            "|    Thai     |    That     |   That    |       The       |      The      |      The      |     The      |\n",
            "+-------------+-------------+-----------+-----------------+---------------+---------------+--------------+\n",
            "|     The     |     The     |    The    |       The       |      The      |      The      |     The      |\n",
            "+-------------+-------------+-----------+-----------------+---------------+---------------+--------------+\n",
            "|     The     |     The     |    The    |       The       |      The      |      The      |     The      |\n",
            "+-------------+-------------+-----------+-----------------+---------------+---------------+--------------+\n",
            "|     The     |     The     |    The    |       The       |      The      |      The      |     The      |\n",
            "+-------------+-------------+-----------+-----------------+---------------+---------------+--------------+\n",
            "|     The     |     The     |    The    |       The       |      The      |      The      |     The      |\n",
            "+-------------+-------------+-----------+-----------------+---------------+---------------+--------------+\n",
            "|     The     |     The     |    The    |       The       |      The      |      The      |     The      |\n",
            "+-------------+-------------+-----------+-----------------+---------------+---------------+--------------+\n",
            "|     The     |     The     |    The    |       The       |      The      |      The      |     The      |\n",
            "+-------------+-------------+-----------+-----------------+---------------+---------------+--------------+\n",
            "|     The     |     The     |    The    |       The       |      The      |      The      |     The      |\n",
            "+-------------+-------------+-----------+-----------------+---------------+---------------+--------------+\n",
            "|     The     |     The     |    The    |      Their      |     Then      |     Then      |    There     |\n",
            "+-------------+-------------+-----------+-----------------+---------------+---------------+--------------+\n",
            "|    They     |    They     |   They    |      They       |     They      |     They      |     They     |\n",
            "+-------------+-------------+-----------+-----------------+---------------+---------------+--------------+\n",
            "|    Third    |    This     |   This    |      This       |     This      |     This      |     This     |\n",
            "+-------------+-------------+-----------+-----------------+---------------+---------------+--------------+\n",
            "|    This     |    This     |   This    |      This       |     This      |     This      |    Thumbs    |\n",
            "+-------------+-------------+-----------+-----------------+---------------+---------------+--------------+\n",
            "|    Thus     | Tigerlilly  |    To     |       To        |      To       |     Toast     |     Took     |\n",
            "+-------------+-------------+-----------+-----------------+---------------+---------------+--------------+\n",
            "|    Total    |     Try     |  Tucson   |  Unfortunately  | Unfortunately | Unfortunately |      Up      |\n",
            "+-------------+-------------+-----------+-----------------+---------------+---------------+--------------+\n",
            "|   Valley    |   Valley    |   Vegas   |      Vegas      |     Vegas     |     Vegas     |    Vegas     |\n",
            "+-------------+-------------+-----------+-----------------+---------------+---------------+--------------+\n",
            "|    Vegas    |    Vegas    |   Vegas   |      Vegas      |     Vegas     |     Vegas     |    Vegas     |\n",
            "+-------------+-------------+-----------+-----------------+---------------+---------------+--------------+\n",
            "|    Vegas    |    Vegas    |   Vegas   |      Vegas      |     Vegas     |     Vegas     |    Vegas     |\n",
            "+-------------+-------------+-----------+-----------------+---------------+---------------+--------------+\n",
            "|    Vegas    |    Vegas    |   Vegas   |   Vegetarian    |  Veggitarian  |     Very      |     Very     |\n",
            "+-------------+-------------+-----------+-----------------+---------------+---------------+--------------+\n",
            "|    Very     |    Very     |   Very    |      Very       |     Very      |    Voodoo     |      We      |\n",
            "+-------------+-------------+-----------+-----------------+---------------+---------------+--------------+\n",
            "|     We      |     We      |    We     |      Went       |     What      |     What      |     When     |\n",
            "+-------------+-------------+-----------+-----------------+---------------+---------------+--------------+\n",
            "|    When     |    When     |   When    | Wienerschnitzel |     Wife      |     Will      |    Worse     |\n",
            "+-------------+-------------+-----------+-----------------+---------------+---------------+--------------+\n",
            "|    Worst    |    Yama     |   Yeah    |     Yelpers     |      You      |     Your      |              |\n",
            "+-------------+-------------+-----------+-----------------+---------------+---------------+--------------+\n",
            "Se identificaron un total de \u001b[32m\u001b[1m517\u001b[0m palabras\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   **Pregunta 11.**  \n",
        "\n",
        "Busca e imprime la secuencia de dos o más palabras que están separadas por un guion, \"-\", sin que tengan espacios en blanco entre ellas.\n",
        "\n",
        "Por ejemplo \"Go-Kart\" sería válido, pero \"Go  -Kart\" o \"Go  -  Kart\" no lo serían.\n",
        "\n",
        "Indica la cantidad de resultados obtenidos."
      ],
      "metadata": {
        "id": "u7nfm4KhMrNW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "patron_11 = r\"\\b\\w+-\\w+\\b\"\n",
        "palabras_11 = []\n",
        "for comentario in comentarios_sin_saltos:\n",
        "  palabras_11.extend(re.findall(patron_11, comentario))\n",
        "\n",
        "imprimir_resultado(palabras_11,3)"
      ],
      "metadata": {
        "id": "OwU-a7eGMsub",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4879b36-da54-4ac7-f411-08225d149ab7"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------+--------------------+----------------+\n",
            "| High-quality |   Service-check    |  been-stepped  |\n",
            "+--------------+--------------------+----------------+\n",
            "|  flat-lined  |   golden-crispy    |   hands-down   |\n",
            "+--------------+--------------------+----------------+\n",
            "|    in-and    |      in-house      |    low-key     |\n",
            "+--------------+--------------------+----------------+\n",
            "| multi-grain  |     must-stop      |  non-customer  |\n",
            "+--------------+--------------------+----------------+\n",
            "|  non-fancy   |      over-hip      |  over-priced   |\n",
            "+--------------+--------------------+----------------+\n",
            "|  over-whelm  |      sit-down      |    sub-par     |\n",
            "+--------------+--------------------+----------------+\n",
            "|    to-go     | tracked-everywhere | under-services |\n",
            "+--------------+--------------------+----------------+\n",
            "Se identificaron un total de \u001b[32m\u001b[1m21\u001b[0m palabras\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   **Pregunta 12.**  \n",
        "\n",
        "Busca e imprime todas las palabras que terminan en \"ing\" o \"ed\".\n",
        "\n",
        "Indica la cantidad de palabras que encontraste de cada una."
      ],
      "metadata": {
        "id": "DEIgl79HMthr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "patron_ing = r\"\\b\\w+ing\\b\"\n",
        "patron_ed = r\"\\b\\w+ed\\b\"\n",
        "\n",
        "# Buscar palabras que coincidan con cada patrón\n",
        "palabras_ing = []\n",
        "palabras_ed = []\n",
        "for comentario in comentarios_sin_saltos:\n",
        "  palabras_ing.extend(re.findall(patron_ing, comentario))\n",
        "  palabras_ed.extend(re.findall(patron_ed, comentario))\n",
        "\n",
        "imprimir_resultado(palabras_ing,7)"
      ],
      "metadata": {
        "id": "I4TSofBMMv9y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6fd542e2-1449-42b7-e091-211fb29b3979"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------+---------------+---------------+---------------+---------------+-------------+---------------+\n",
            "|    Coming     | Disappointing |  Everything   |  Everything   |  Everything   | Everything  |  Everything   |\n",
            "+---------------+---------------+---------------+---------------+---------------+-------------+---------------+\n",
            "|  Everything   |  Interesting  |    Nothing    |  Outstanding  |    Paying     |   Pricing   |    amazing    |\n",
            "+---------------+---------------+---------------+---------------+---------------+-------------+---------------+\n",
            "|    amazing    |    amazing    |    amazing    |    amazing    |    amazing    |   amazing   |    amazing    |\n",
            "+---------------+---------------+---------------+---------------+---------------+-------------+---------------+\n",
            "|    amazing    |    amazing    |    amazing    |    amazing    |    amazing    |   amazing   |    amazing    |\n",
            "+---------------+---------------+---------------+---------------+---------------+-------------+---------------+\n",
            "|    amazing    |    amazing    |    amazing    |    amazing    |    amazing    |   amazing   |    amazing    |\n",
            "+---------------+---------------+---------------+---------------+---------------+-------------+---------------+\n",
            "|    amazing    |    amazing    |   annoying    |   anything    |   anything    |  appalling  |   appealing   |\n",
            "+---------------+---------------+---------------+---------------+---------------+-------------+---------------+\n",
            "|   arriving    |    asking     |     being     |     being     |     being     |    being    |     being     |\n",
            "+---------------+---------------+---------------+---------------+---------------+-------------+---------------+\n",
            "|     being     |     being     |     being     |     being     |     being     |    being    |     being     |\n",
            "+---------------+---------------+---------------+---------------+---------------+-------------+---------------+\n",
            "|     being     |     being     |    boring     |     bring     |     bring     |    bring    |     bring     |\n",
            "+---------------+---------------+---------------+---------------+---------------+-------------+---------------+\n",
            "|     bring     |   building    |   building    |    buying     |    caring     |  changing   |   charming    |\n",
            "+---------------+---------------+---------------+---------------+---------------+-------------+---------------+\n",
            "|   climbing    |    coming     |    coming     |    coming     |    coming     |   coming    |    coming     |\n",
            "+---------------+---------------+---------------+---------------+---------------+-------------+---------------+\n",
            "|    coming     |    coming     |    coming     |  considering  |  considering  | considering |  considering  |\n",
            "+---------------+---------------+---------------+---------------+---------------+-------------+---------------+\n",
            "|    cooking    |   cramming    |    craving    |    dealing    |    dealing    | describing  |    dining     |\n",
            "+---------------+---------------+---------------+---------------+---------------+-------------+---------------+\n",
            "|    dining     |    dining     |    dining     |    dining     |    dining     |   dining    |    dining     |\n",
            "+---------------+---------------+---------------+---------------+---------------+-------------+---------------+\n",
            "|    dipping    | disappointing | disappointing | disappointing | disappointing | disgusting  |  disgusting   |\n",
            "+---------------+---------------+---------------+---------------+---------------+-------------+---------------+\n",
            "|     doing     |    drawing    |   dressing    |   dressing    |   dressing    |  drinking   |   dripping    |\n",
            "+---------------+---------------+---------------+---------------+---------------+-------------+---------------+\n",
            "|    driving    |    during     |    during     |    during     |    during     |   during    |    eating     |\n",
            "+---------------+---------------+---------------+---------------+---------------+-------------+---------------+\n",
            "|    eating     |    eating     |    eating     |    eating     |    eating     |   eating    |    eating     |\n",
            "+---------------+---------------+---------------+---------------+---------------+-------------+---------------+\n",
            "|    editing    |    evening    |    evening    |  everything   |  everything   | everything  |  everything   |\n",
            "+---------------+---------------+---------------+---------------+---------------+-------------+---------------+\n",
            "|  everything   |  everything   |   exceeding   | experiencing  |    falling    |   feeling   |    feeling    |\n",
            "+---------------+---------------+---------------+---------------+---------------+-------------+---------------+\n",
            "|    filling    |    filling    |   flirting    |  forgetting   |   freaking    |   fucking   |    getting    |\n",
            "+---------------+---------------+---------------+---------------+---------------+-------------+---------------+\n",
            "|    getting    |    getting    |    getting    |    getting    |    getting    |   getting   |    getting    |\n",
            "+---------------+---------------+---------------+---------------+---------------+-------------+---------------+\n",
            "|    giving     |     going     |     going     |     going     |     going     |    going    |     going     |\n",
            "+---------------+---------------+---------------+---------------+---------------+-------------+---------------+\n",
            "|     going     |     going     |     going     |     going     |     going     |    going    |     going     |\n",
            "+---------------+---------------+---------------+---------------+---------------+-------------+---------------+\n",
            "|     going     |     going     |     going     |     going     |     going     |  handling   |   hankering   |\n",
            "+---------------+---------------+---------------+---------------+---------------+-------------+---------------+\n",
            "|    having     |    hoping     |   including   |  interesting  |  interesting  |  inviting   |    judging    |\n",
            "+---------------+---------------+---------------+---------------+---------------+-------------+---------------+\n",
            "|    lacking    |    lacking    |    lacking    |    letting    |   lighting    |   liking    |    living     |\n",
            "+---------------+---------------+---------------+---------------+---------------+-------------+---------------+\n",
            "|    looking    |    looking    |    loving     |  maintaining  |    making     |   missing   |    nothing    |\n",
            "+---------------+---------------+---------------+---------------+---------------+-------------+---------------+\n",
            "|    nothing    |    nothing    |    nothing    |    nothing    |    nothing    |  ordering   |  outshining   |\n",
            "+---------------+---------------+---------------+---------------+---------------+-------------+---------------+\n",
            "|  outstanding  |  outstanding  |    playing    |    playing    |   poisoning   |  preparing  |   preparing   |\n",
            "+---------------+---------------+---------------+---------------+---------------+-------------+---------------+\n",
            "|    pricing    |   providing   |    putting    |    rating     |    raving     |   reading   | recommending  |\n",
            "+---------------+---------------+---------------+---------------+---------------+-------------+---------------+\n",
            "|   redeeming   |  refreshing   |   returning   |   reviewing   |  revisiting   |  rotating   |    running    |\n",
            "+---------------+---------------+---------------+---------------+---------------+-------------+---------------+\n",
            "|    running    |    running    |    running    |   satifying   |  satisfying   | satisfying  |    saving     |\n",
            "+---------------+---------------+---------------+---------------+---------------+-------------+---------------+\n",
            "|    saying     |   seasoning   |    seating    |    seating    |    seating    |   serving   |    serving    |\n",
            "+---------------+---------------+---------------+---------------+---------------+-------------+---------------+\n",
            "|    setting    |    setting    |    setting    |   shopping    |    sitting    |   sitting   |   something   |\n",
            "+---------------+---------------+---------------+---------------+---------------+-------------+---------------+\n",
            "|   something   |   something   |   something   |   sporting    |    spring     |  starving   |    staying    |\n",
            "+---------------+---------------+---------------+---------------+---------------+-------------+---------------+\n",
            "|    staying    |    talking    |     thing     |     thing     |     thing     |    thing    |     thing     |\n",
            "+---------------+---------------+---------------+---------------+---------------+-------------+---------------+\n",
            "|     thing     |     thing     |     thing     |   thinking    |    trying     |   trying    | underwhelming |\n",
            "+---------------+---------------+---------------+---------------+---------------+-------------+---------------+\n",
            "| underwhelming | unsatisfying  |   upgrading   |   venturing   |    waiting    |   waiting   |    waiting    |\n",
            "+---------------+---------------+---------------+---------------+---------------+-------------+---------------+\n",
            "|    waiting    |    waiting    |    wasting    |    wasting    |    working    |   writing   |               |\n",
            "+---------------+---------------+---------------+---------------+---------------+-------------+---------------+\n",
            "Se identificaron un total de \u001b[32m\u001b[1m279\u001b[0m palabras\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "imprimir_resultado(palabras_ed,7)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LyBhzb9GDchv",
        "outputId": "0c840546-c43b-49ca-ae79-e3738e749bfa"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------+--------------+---------------+--------------+--------------+--------------+--------------+\n",
            "|    Based     |    Cooked    | Disappointed  |    Loved     |    Loved     |   Ordered    |   Ordered    |\n",
            "+--------------+--------------+---------------+--------------+--------------+--------------+--------------+\n",
            "|   Ordered    |  Overpriced  |    Stopped    |   Stopped    |    Tasted    |    Tried     |    Waited    |\n",
            "+--------------+--------------+---------------+--------------+--------------+--------------+--------------+\n",
            "|    Waited    | acknowledged | acknowledged  |    added     |    added     | anticipated  |   arrived    |\n",
            "+--------------+--------------+---------------+--------------+--------------+--------------+--------------+\n",
            "|   arrived    |   arrived    |     asked     |    asked     |    asked     |    asked     |    asked     |\n",
            "+--------------+--------------+---------------+--------------+--------------+--------------+--------------+\n",
            "|   attached   |   avoided    |    boiled     |    burned    |   charged    |   cheated    |   checked    |\n",
            "+--------------+--------------+---------------+--------------+--------------+--------------+--------------+\n",
            "|   checked    |   checked    |    claimed    |    closed    | constructed  |  contained   |    cooked    |\n",
            "+--------------+--------------+---------------+--------------+--------------+--------------+--------------+\n",
            "|    cooked    |    cooked    |    cooked     |    cooked    |    cooked    |   covered    |   decided    |\n",
            "+--------------+--------------+---------------+--------------+--------------+--------------+--------------+\n",
            "|   decided    |  decorated   |   decorated   |  dedicated   |   desired    | disappointed | disappointed |\n",
            "+--------------+--------------+---------------+--------------+--------------+--------------+--------------+\n",
            "| disappointed | disappointed | disappointed  | disappointed | disappointed | disappointed | disappointed |\n",
            "+--------------+--------------+---------------+--------------+--------------+--------------+--------------+\n",
            "| disappointed | disappointed | disappointed  | disappointed | disappointed | disappointed | disappointed |\n",
            "+--------------+--------------+---------------+--------------+--------------+--------------+--------------+\n",
            "| disappointed | disappointed |   disgusted   | disrespected | disrespected |   dreamed    |   drenched   |\n",
            "+--------------+--------------+---------------+--------------+--------------+--------------+--------------+\n",
            "|   dressed    |    dried     |    dropped    |    dusted    |    ended     |    ended     |   enjoyed    |\n",
            "+--------------+--------------+---------------+--------------+--------------+--------------+--------------+\n",
            "|   enjoyed    |   enjoyed    |    enjoyed    |   enjoyed    |    ensued    |   expanded   |   expected   |\n",
            "+--------------+--------------+---------------+--------------+--------------+--------------+--------------+\n",
            "|   expected   |   expected   |  experienced  |     eyed     |   figured    |   flavored   |   focused    |\n",
            "+--------------+--------------+---------------+--------------+--------------+--------------+--------------+\n",
            "|   focused    |    fried     |     fried     |    fried     |    fried     |    fried     |  frustrated  |\n",
            "+--------------+--------------+---------------+--------------+--------------+--------------+--------------+\n",
            "|   greeted    |   greeted    |    grilled    |   grilled    |   grossed    |   grossed    |    handed    |\n",
            "+--------------+--------------+---------------+--------------+--------------+--------------+--------------+\n",
            "|   handled    |   happened   |     hated     |    helped    |    helped    | highlighted  |    hooked    |\n",
            "+--------------+--------------+---------------+--------------+--------------+--------------+--------------+\n",
            "|  humiliated  |     iced     |    ignored    |   imagined   |  impressed   |  impressed   |  impressed   |\n",
            "+--------------+--------------+---------------+--------------+--------------+--------------+--------------+\n",
            "|  impressed   |  impressed   |   impressed   |  impressed   |  impressed   |   included   |   inspired   |\n",
            "+--------------+--------------+---------------+--------------+--------------+--------------+--------------+\n",
            "|   insulted   |   insulted   |   insulted    |    lacked    |    lacked    |    lacked    |    liked     |\n",
            "+--------------+--------------+---------------+--------------+--------------+--------------+--------------+\n",
            "|    liked     |    liked     |    limited    |    lined     |    listed    |    lived     |   located    |\n",
            "+--------------+--------------+---------------+--------------+--------------+--------------+--------------+\n",
            "|    looked    |    looked    |     loved     |    loved     |    loved     |    loved     |    loved     |\n",
            "+--------------+--------------+---------------+--------------+--------------+--------------+--------------+\n",
            "|    loved     |   managed    |    melted     |    missed    |    mixed     |  mortified   |     need     |\n",
            "+--------------+--------------+---------------+--------------+--------------+--------------+--------------+\n",
            "|     need     |     need     |    needed     |    needed    |   offered    |    opened    |   opposed    |\n",
            "+--------------+--------------+---------------+--------------+--------------+--------------+--------------+\n",
            "|   ordered    |   ordered    |    ordered    |   ordered    |   ordered    |   ordered    |   ordered    |\n",
            "+--------------+--------------+---------------+--------------+--------------+--------------+--------------+\n",
            "|   ordered    |   ordered    |    ordered    |   ordered    |  overcooked  |  overcooked  |  overpriced  |\n",
            "+--------------+--------------+---------------+--------------+--------------+--------------+--------------+\n",
            "|  overpriced  |  overpriced  |  overpriced   |  overpriced  | overwhelmed  | overwhelmed  |    owned     |\n",
            "+--------------+--------------+---------------+--------------+--------------+--------------+--------------+\n",
            "|    packed    |    passed    |    passed     |  performed   |   perpared   |  petrified   |    placed    |\n",
            "+--------------+--------------+---------------+--------------+--------------+--------------+--------------+\n",
            "|   pleased    |   pleased    |    pleased    |    poured    |   powdered   |   prepared   |    priced    |\n",
            "+--------------+--------------+---------------+--------------+--------------+--------------+--------------+\n",
            "|    priced    |    priced    |  privileged   |  proclaimed  |   provided   |    pulled    |    pulled    |\n",
            "+--------------+--------------+---------------+--------------+--------------+--------------+--------------+\n",
            "|    pulled    |    puréed    |   qualified   |    rated     |    rated     |   realized   |   realized   |\n",
            "+--------------+--------------+---------------+--------------+--------------+--------------+--------------+\n",
            "|   received   |   received   |   received    | recommended  | recommended  | recommended  |     red      |\n",
            "+--------------+--------------+---------------+--------------+--------------+--------------+--------------+\n",
            "|     red      |  refrained   |    refried    |   refused    |   reheated   |   relaxed    |  relocated   |\n",
            "+--------------+--------------+---------------+--------------+--------------+--------------+--------------+\n",
            "|   reminded   | replenished  |   requested   |   returned   |    ripped    |    ripped    |   roasted    |\n",
            "+--------------+--------------+---------------+--------------+--------------+--------------+--------------+\n",
            "|   roasted    |    rolled    |    rushed     |  satisfied   |   screwed    |   seasoned   |   seasoned   |\n",
            "+--------------+--------------+---------------+--------------+--------------+--------------+--------------+\n",
            "|    seated    |    seated    |    seated     |    seated    |    seated    |    seemed    |    seemed    |\n",
            "+--------------+--------------+---------------+--------------+--------------+--------------+--------------+\n",
            "|    served    |    served    |    served     |    served    |    served    |    served    |   shocked    |\n",
            "+--------------+--------------+---------------+--------------+--------------+--------------+--------------+\n",
            "|    showed    |    sliced    |    sliced     |   smeared    |   smelled    |   started    |   started    |\n",
            "+--------------+--------------+---------------+--------------+--------------+--------------+--------------+\n",
            "|    stayed    |    stayed    |    stepped    |   stepped    |   stuffed    |   stuffed    |    sucked    |\n",
            "+--------------+--------------+---------------+--------------+--------------+--------------+--------------+\n",
            "|    sucked    |    sucked    |    sucked     |   supposed   |  surprised   |   tailored   |    tasted    |\n",
            "+--------------+--------------+---------------+--------------+--------------+--------------+--------------+\n",
            "|    tasted    |    tasted    |    tasted     |    tasted    |    tasted    |    tasted    |   thrilled   |\n",
            "+--------------+--------------+---------------+--------------+--------------+--------------+--------------+\n",
            "|   toasted    |   toasted    |    touched    |   tracked    |   treated    |   treated    |   treated    |\n",
            "+--------------+--------------+---------------+--------------+--------------+--------------+--------------+\n",
            "|    tried     |    tried     |     tried     |    tried     |    tried     |    tried     |   trimmed    |\n",
            "+--------------+--------------+---------------+--------------+--------------+--------------+--------------+\n",
            "| undercooked  | undercooked  | unexperienced |  uninspired  |  untoasted   |  unwrapped   |   uploaded   |\n",
            "+--------------+--------------+---------------+--------------+--------------+--------------+--------------+\n",
            "|     used     |     used     |     used      |   visited    |   vomited    |    voted     |    waited    |\n",
            "+--------------+--------------+---------------+--------------+--------------+--------------+--------------+\n",
            "|    waited    |    waited    |    waited     |    waited    |    waited    |    waited    |    waited    |\n",
            "+--------------+--------------+---------------+--------------+--------------+--------------+--------------+\n",
            "|    walked    |    walked    |    walked     |    wanted    |    wanted    |    wanted    |    wasted    |\n",
            "+--------------+--------------+---------------+--------------+--------------+--------------+--------------+\n",
            "|   watched    |   watched    |    watched    |   watered    |  witnessed   |   wrapped    |              |\n",
            "+--------------+--------------+---------------+--------------+--------------+--------------+--------------+\n",
            "Se identificaron un total de \u001b[32m\u001b[1m335\u001b[0m palabras\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Parte 3. Proceso de limpieza.**"
      ],
      "metadata": {
        "id": "70StdqAZa9E9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   **Pregunta 13.**  \n",
        "\n",
        "Ahora realiza un proceso de limpieza del corpus que incluya los siguientes procesos:\n",
        "\n",
        "*   Solo se deben considerar caracteres alfabéticos. Es decir, se eliminan todos los signos de puntuación y caracteres especiales.\n",
        "*   Todos los caracteres alfabéticos se transforman a minúsculas.\n",
        "*   Se deben eliminar todos los espacios en blanco adicionales que se puedan encontrar en cada comentario.\n",
        "\n",
        "Al finalizar dicho proceso de limpieza, imprime el resultado de los primeros 10 comentarios resultantes.\n",
        "   \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xaDUFXHrMvX2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def limpiar_texto(texto):\n",
        "    comentarios_limpios = []\n",
        "    for comentario in texto:\n",
        "        comentario_limpio = re.sub(r'[^a-zA-Z\\s]', '', comentario)  # Solo mantener caracteres alfabéticos (y espacios)\n",
        "        comentario_limpio = comentario_limpio.lower()               # Convertir a minúsculas\n",
        "        comentario_limpio = re.sub(r'\\s+', ' ', comentario_limpio)  # Eliminar espacios en blanco adicionales\n",
        "        comentarios_limpios.append(comentario_limpio)\n",
        "    return comentarios_limpios\n",
        ""
      ],
      "metadata": {
        "id": "K3kQzPOPMx0w"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "comentario_limpio = limpiar_texto(comentarios_sin_saltos)\n",
        "\n",
        "for comentario in comentario_limpio[:10]:\n",
        "    print(comentario)"
      ],
      "metadata": {
        "id": "mYEDlHSFMyJN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d90019bc-5fae-4645-cbed-c5db09f384d1"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "wow loved this place\n",
            "crust is not good\n",
            "not tasty and the texture was just nasty\n",
            "stopped by during the late may bank holiday off rick steve recommendation and loved it\n",
            "the selection on the menu was great and so were the prices\n",
            "now i am getting angry and i want my damn pho\n",
            "honeslty it didnt taste that fresh\n",
            "the potatoes were like rubber and you could tell they had been made up ahead of time being kept under a warmer\n",
            "the fries were great too\n",
            "a great touch\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   **Pregunta 14.**  \n",
        "\n",
        "Con el resultado de la limpieza obtenido en la pregunta anterior, realiza ahora un proceso de tokenización por palabras del corpus.\n",
        "\n",
        "Es decir, al final de este proceso de tokenización, debes tener como resultado una lista de listas, donde cada comentario estará tokenizado por palabras.\n",
        "\n",
        "Al terminar calcula el total de tokens obtenido en todo el corpus."
      ],
      "metadata": {
        "id": "WZwEhg2lUSAX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "comentarios_tokenizados = [comentario.split() for comentario in comentario_limpio]\n",
        "total_tokens = sum(len(comentario) for comentario in comentarios_tokenizados)\n",
        "\n",
        "print(f\"Total de tokens: {total_tokens}\")"
      ],
      "metadata": {
        "id": "kbAL9-v0V-jx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26f43c58-1b25-4d98-d294-785f4c925018"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total de tokens: 10777\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   **Pregunta 15.**  \n",
        "\n",
        "Finalmente, en este ejercicio definiremos nuestro conjunto de palabras \"stopwords\", las cuales deberás eliminar de todo el corpus.\n",
        "\n",
        "Recuerda que ejemplos de stopwords son artículos, adverbios, conectivos, etcétera, que tienen frecuencias de aparición muy altas en cualquier documento, pero que no brindan mucho significado en cuanto al significado de un enunciado.\n",
        "\n",
        "Con base a la lista de stopwords que se te proporciona, realiza un proceso de limpieza eliminando todas estas palabras del corpus obtenido en el ejercicio anterior.\n",
        "\n",
        "Obtener cuántos tokens/palabras quedan finalmente en todo el corpus.\n",
        "\n",
        "Obtener cuántos de estos tokens/palabras son diferentes, es decir, cuántos tokens únicos tendrá lo que llamaremos más adelante nuestro vocabulario."
      ],
      "metadata": {
        "id": "EFeu0OJ7WDPD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Considera la siguiente lista como tu conjunto de stopwords:\n",
        "mis_stopwords = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'should', 'now', 'll']"
      ],
      "metadata": {
        "id": "6FP4FF3KXGxm"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "comentarios_sin_stopwords = []\n",
        "\n",
        "for comentario in comentarios_tokenizados:\n",
        "  comentarios_sin_stopwords.append([palabra for palabra in comentario if palabra not in mis_stopwords])\n",
        "\n",
        "total_tokens_restantes = sum(len(comentario) for comentario in comentarios_sin_stopwords)\n",
        "\n",
        "print(f\"Total de tokens restantes: \\033[32m\\033[1m{total_tokens_restantes}\\033[0m\")\n",
        "\n",
        "# Calcular tokens únicos (vocabulario)\n",
        "tokens_unicos = set([palabra for comentario in comentarios_sin_stopwords for palabra in comentario])\n",
        "total_tokens_unicos = len(tokens_unicos)\n",
        "print(f\"Total de tokens únicos (vocabulario): \\033[32m\\033[1m{total_tokens_unicos}\\033[0m\")"
      ],
      "metadata": {
        "id": "CD8yjyq1ZrwY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7d8c205-2f0b-4d46-b738-7470e599c872"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total de tokens restantes: \u001b[32m\u001b[1m5776\u001b[0m\n",
            "Total de tokens únicos (vocabulario): \u001b[32m\u001b[1m1941\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   **Comentarios**\n",
        "\n",
        "Incluye finalmente tus comentarios de la actividad."
      ],
      "metadata": {
        "id": "NDbKkuxRbLoX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<< incluye aquí tus comentarios >>"
      ],
      "metadata": {
        "id": "o7fzbvqVbUGr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Fin de la Actividad de la semana 2.**"
      ],
      "metadata": {
        "id": "PHaKw_6Ldbaf"
      }
    }
  ]
}