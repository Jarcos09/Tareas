{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**Maestría en Inteligencia Artificial Aplicada**\n",
        "##**Curso: Procesamiento de Lenguaje Natural (NLP)**\n",
        "###Tecnológico de Monterrey\n",
        "###Prof Luis Eduardo Falcón Morales\n",
        "\n",
        "## **Adtividad de la Semana 02**\n",
        "###**Introducción al procesamiento de texto.**"
      ],
      "metadata": {
        "id": "759SG4TyfbUn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En esta actividad deberás utilizar los datos del siguiente archivo que se encuentra en Canvas:\n",
        "\n",
        "MNA_NLP_semana_02_Actividad_datos.txt\n",
        "\n",
        "El archivo contiene comentarios en inglés sobre servicios de comida de la página de Yelp: https://www.yelp.com/ .\n",
        "\n",
        "Son mil comentarios y forman parte del conjunto de datos que se encuentra en el Machine Learning Repository de la UCI, llamado \"Sentiment Labelled Sentences\": https://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences#\n"
      ],
      "metadata": {
        "id": "6ue1YAKx3XDo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Parte 1. Cargamos los datos.**   "
      ],
      "metadata": {
        "id": "Zj-h4drXD-X9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cargar los datos del archivo indicado y obtener una lista de longitud de 1000 strings/comentarios.\n",
        "\n",
        "Por el momento solamente requerimos las bibliotecas de Numpy y re, para el manejo de los arreglos y de las expresiones regulares en Python.\n",
        "\n",
        "En particular, no necesitarás en esta actividad la biblioteca de Pandas.\n",
        "\n",
        "###**NOTA: En esta actividad no debes importar nada más, con estas dos bibliotecas será *suficiente*.**"
      ],
      "metadata": {
        "id": "BY6yifxscfrx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np       # importamos Numpy para el manejo de los arreglos.\n",
        "import re                # importamos re para el manejo de las expresiones regulares.\n",
        "import gdown             # importamos gdown para descargar el archivo de trabajo\n",
        "import os                # importamos os para manejo de archivos y directorios\n",
        "import tabulate as tab   # importamos tabulate para impresión de tablas"
      ],
      "metadata": {
        "id": "OJ26dAfhdFnf"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs('datos', exist_ok=True)\n",
        "datos_url = 'https://drive.google.com/uc?id=1qVQ5sn901fDCQMRP_NpaHEoTEEOlrfM3'\n",
        "output = 'datos/actividad_datos.txt'\n",
        "\n",
        "print(\"Descargando datos\")\n",
        "gdown.download(datos_url, output, quiet=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wqc0R1HMRP-x",
        "outputId": "03757559-3804-41fb-85dd-494abf99b8b9"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Descargando datos\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1qVQ5sn901fDCQMRP_NpaHEoTEEOlrfM3\n",
            "To: /home/jarcos/datos/actividad_datos.txt\n",
            "100%|█████████████████████████████████████████████████████████████████████| 59.9k/59.9k [00:00<00:00, 510kB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'datos/actividad_datos.txt'"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ejecuta las siguientes instrucciones para cargar la información del achivo dado:\n",
        "\n",
        "with open('datos/actividad_datos.txt',        # puedes actualizar la ruta a tu archivo, en dado caso.\n",
        "          mode='r',                           # abrimos el archivo en modo lectura.\n",
        "          ) as f:\n",
        "    docs = f.readlines()                      # separamos cada comentario por líneas\n",
        "\n",
        "f.close()                         # ya que tenemos la información en la variable docs, cerramos el archivo"
      ],
      "metadata": {
        "id": "QHUmJyjDdGNP"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(docs) == list   # Verifica que tu variable \"docs\" es una lista"
      ],
      "metadata": {
        "id": "L6WzrSrodG-Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86798448-842d-495e-ee79-acf3c0de92df"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(docs)==1000  # verifica que la longitud de \"docs\" es de mil comentarios."
      ],
      "metadata": {
        "id": "QIK1u9WS2FtS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "991f50d5-494e-404e-a507-b224aa728adb"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "docs[0:10]     # observa algunos de los primeros comentarios"
      ],
      "metadata": {
        "id": "9AMLIfQvJqNZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f14a1479-81c5-4bfc-faa4-57810040c6f7"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Wow... Loved this place.\\n',\n",
              " 'Crust is not good.\\n',\n",
              " 'Not tasty and the texture was just nasty.\\n',\n",
              " 'Stopped by during the late May bank holiday off Rick Steve recommendation and loved it.\\n',\n",
              " 'The selection on the menu was great and so were the prices.\\n',\n",
              " 'Now I am getting angry and I want my damn pho.\\n',\n",
              " \"Honeslty it didn't taste THAT fresh.)\\n\",\n",
              " 'The potatoes were like rubber and you could tell they had been made up ahead of time being kept under a warmer.\\n',\n",
              " 'The fries were great too.\\n',\n",
              " 'A great touch.\\n']"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Parte 2: sección de preguntas (regex).**   \n"
      ],
      "metadata": {
        "id": "k_ewoagic5jc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Instrucciones:**\n",
        "\n",
        "###**A continuación deberás contestar cada una de las preguntas que te piden usando expresiones regulares (regex).**\n",
        "\n",
        "###**Por el momento no hay restricción en cuanto al número de líneas de código que agregues, pero trata de incluir las mínimas posibles.**"
      ],
      "metadata": {
        "id": "X-eMJa3DFCIV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   **Pregunta 1.**\n",
        "\n",
        "Busca y elimina todos los saltos de línea '\\n' que se encuentran al final de cada comentario.\n",
        "\n",
        "Una vez finalizado, imprime los primeros 10 comentarios del resultado obtenido.\n"
      ],
      "metadata": {
        "id": "78nJMemzn5a5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "comentarios_sin_saltos = [re.sub(r'\\n$', '', registro) for registro in docs]\n",
        "for comentario in comentarios_sin_saltos[:10]:\n",
        "    print(comentario)"
      ],
      "metadata": {
        "id": "PwbYYIuZn8pE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae809b42-b333-41d5-e1be-adbac39f14a1"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wow... Loved this place.\n",
            "Crust is not good.\n",
            "Not tasty and the texture was just nasty.\n",
            "Stopped by during the late May bank holiday off Rick Steve recommendation and loved it.\n",
            "The selection on the menu was great and so were the prices.\n",
            "Now I am getting angry and I want my damn pho.\n",
            "Honeslty it didn't taste THAT fresh.)\n",
            "The potatoes were like rubber and you could tell they had been made up ahead of time being kept under a warmer.\n",
            "The fries were great too.\n",
            "A great touch.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   **Pregunta 2.**  \n",
        "\n",
        "Busca e imprime todas las palabras que terminan con dos o más signos de admiración seguidos, por ejemplo \"!!!\".\n",
        "\n",
        "Debes imprimir tanto la palabra como la totalidad de signos de admiración que le siguen.\n",
        "\n",
        "Indica cuántos resultados obtuviste.\n",
        "\n"
      ],
      "metadata": {
        "id": "VWeKQC93ctEo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def imprimir_resultado(palabras,columnas):\n",
        "  filas = []\n",
        "  ordenados = sorted(palabras)\n",
        "  for i in range(0, len(ordenados), columnas):\n",
        "        fila = ordenados[i:i + columnas]\n",
        "\n",
        "        while len(fila) < columnas:\n",
        "            fila.append(\"\")\n",
        "        filas.append(fila)\n",
        "\n",
        "  print(tab.tabulate(filas, tablefmt=\"grid\", stralign=\"center\"))\n",
        "  print(f\"Se identificaron un total de \\033[32m\\033[1m{len(palabras)}\\033[0m palabras\")"
      ],
      "metadata": {
        "id": "ixtW-jri6upD"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "palabras_con_exclamaciones = []\n",
        "for comentario in comentarios_sin_saltos:\n",
        "    palabras_con_exclamaciones.extend(re.findall(r'\\b\\w+!{2,}', comentario))\n",
        "\n",
        "imprimir_resultado(palabras_con_exclamaciones,3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LwO2Gz0Y7vs1",
        "outputId": "87f5b9ca-5736-4294-dbc0-06a60590cff8"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------+----------------------------+---------------------+\n",
            "| APPETIZERS!!!  |        DELICIOUS!!         |     FLY!!!!!!!!     |\n",
            "+----------------+----------------------------+---------------------+\n",
            "| Firehouse!!!!! |            Up!!            |      amazing!!      |\n",
            "+----------------+----------------------------+---------------------+\n",
            "|   amazing!!!   | amazing!!!!!!!!!!!!!!!!!!! |      awesome!!      |\n",
            "+----------------+----------------------------+---------------------+\n",
            "|  biscuits!!!   |         buffet!!!          |    delicious!!!     |\n",
            "+----------------+----------------------------+---------------------+\n",
            "| disappointed!! |      disappointing!!!      |        dry!!        |\n",
            "+----------------+----------------------------+---------------------+\n",
            "|     far!!      |           good!!           | great!!!!!!!!!!!!!! |\n",
            "+----------------+----------------------------+---------------------+\n",
            "|    here!!!     |           it!!!!           |     otherwise!!     |\n",
            "+----------------+----------------------------+---------------------+\n",
            "|    packed!!    |    shawarrrrrrma!!!!!!     |     steak!!!!!      |\n",
            "+----------------+----------------------------+---------------------+\n",
            "|    style!!     |          yucky!!!          |                     |\n",
            "+----------------+----------------------------+---------------------+\n",
            "Se identificaron un total de \u001b[32m\u001b[1m26\u001b[0m palabras\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   **Pregunta 3.**  \n",
        "\n",
        "Busca e imprime todas las palabras que están escritas totalmente en mayúsculas. Cada coincidencia debe ser una sola palabra.\n",
        "\n",
        "Indica cuántas palabras encontraste.\n",
        "\n"
      ],
      "metadata": {
        "id": "-s3okBqL96TT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "set_mayusculas = set()\n",
        "for comentario in comentarios_sin_saltos:\n",
        "    set_mayusculas.update(re.findall(r'\\b[A-Z]+\\b', comentario))\n",
        "\n",
        "palabras_mayusculas = list(set_mayusculas)\n",
        "\n",
        "imprimir_resultado(palabras_mayusculas,6)"
      ],
      "metadata": {
        "id": "yKHJkZKo_nW5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fcab6326-cce1-4ba0-96e1-8da31b4c348b"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+---------------+---------------+------------+-----------+------------+\n",
            "|     A     |     AGAIN     |      ALL      |     AN     |    AND    | APPETIZERS |\n",
            "+-----------+---------------+---------------+------------+-----------+------------+\n",
            "|   AVOID   |     AYCE      |      AZ       |    BACK    |   BARE    |  BARGAIN   |\n",
            "+-----------+---------------+---------------+------------+-----------+------------+\n",
            "|    BBQ    |     BEST      |    BETTER     |  BITCHES   |   BLAND   | CONCLUSION |\n",
            "+-----------+---------------+---------------+------------+-----------+------------+\n",
            "| DELICIOUS | ESTABLISHMENT |     EVER      | EXPERIENCE | FANTASTIC |   FLAVOR   |\n",
            "+-----------+---------------+---------------+------------+-----------+------------+\n",
            "|    FLY    |    FORWARD    |   FREEZING    |     FS     |    GC     |     GO     |\n",
            "+-----------+---------------+---------------+------------+-----------+------------+\n",
            "|   GREAT   |      HAD      |     HANDS     |  HAPPENED  |   HAVE    |    HOUR    |\n",
            "+-----------+---------------+---------------+------------+-----------+------------+\n",
            "|     I     |      IN       | INCONSIDERATE |     IT     |   LEGIT   |   LOVED    |\n",
            "+-----------+---------------+---------------+------------+-----------+------------+\n",
            "|     M     |  MANAGEMENT   |     MANY      |    MGM     |   MUST    |   NASTY    |\n",
            "+-----------+---------------+---------------+------------+-----------+------------+\n",
            "|   NEVER   |      NO       |     NONE      |    NOT     |    NOW    |    NYC     |\n",
            "+-----------+---------------+---------------+------------+-----------+------------+\n",
            "|    OF     |      OK       |      OMG      | OVERPRICED |  OWNERS   |   PEOPLE   |\n",
            "+-----------+---------------+---------------+------------+-----------+------------+\n",
            "|  PERFECT  |     REAL      |    REALLY     |     RI     |   RUDE    |  SCREAMS   |\n",
            "+-----------+---------------+---------------+------------+-----------+------------+\n",
            "|  SHOULD   |     STALE     |     STEP      |     T      |   THAT    |    THE     |\n",
            "+-----------+---------------+---------------+------------+-----------+------------+\n",
            "|   THIS    |     TIME      |     TOLD      |   TOTAL    |    TV     |   UNREAL   |\n",
            "+-----------+---------------+---------------+------------+-----------+------------+\n",
            "|   VERY    |     WASTE     |      WAY      |    WEAK    |   WHAT    |    WILL    |\n",
            "+-----------+---------------+---------------+------------+-----------+------------+\n",
            "|   WORST   |               |               |            |           |            |\n",
            "+-----------+---------------+---------------+------------+-----------+------------+\n",
            "Se identificaron un total de \u001b[32m\u001b[1m85\u001b[0m palabras\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   **Pregunta 4.**  \n",
        "\n",
        "Busca e imprime los comentarios en donde todos los caracteres alfabéticos (letras) están en mayúsculas.\n",
        "\n",
        "Cada coincidencia encontrada debe ser todo el comentario/enunciado.\n",
        "\n",
        "Indica cuántos resultados obtuviste.\n"
      ],
      "metadata": {
        "id": "GX8eYyDoMZma"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "comentarios_mayusculas = []\n",
        "\n",
        "for comentario in comentarios_sin_saltos:\n",
        "    if re.fullmatch(r'[A-Z\\s\\W]+', comentario):\n",
        "        comentarios_mayusculas.append(comentario)\n",
        "\n",
        "imprimir_resultado(comentarios_mayusculas,1)"
      ],
      "metadata": {
        "id": "K8VuZxvTMYj6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e220074-a0ea-42f1-903a-b5ce547ed7e2"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------------------------------------------------------------+\n",
            "|                      AVOID THIS ESTABLISHMENT!                       |\n",
            "+----------------------------------------------------------------------+\n",
            "|                             DELICIOUS!!                              |\n",
            "+----------------------------------------------------------------------+\n",
            "|                   RUDE & INCONSIDERATE MANAGEMENT.                   |\n",
            "+----------------------------------------------------------------------+\n",
            "|                         TOTAL WASTE OF TIME.                         |\n",
            "+----------------------------------------------------------------------+\n",
            "| WILL NEVER EVER GO BACK AND HAVE TOLD MANY PEOPLE WHAT HAD HAPPENED. |\n",
            "+----------------------------------------------------------------------+\n",
            "Se identificaron un total de \u001b[32m\u001b[1m5\u001b[0m palabras\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   **Pregunta 5.**  \n",
        "\n",
        "Busca e imprime todas las palabras que tengan una vocal acentuada, del tipo á, é, í, ó, ú.\n",
        "\n",
        "Indica cuántos resultados obtuviste."
      ],
      "metadata": {
        "id": "a1i6qv7-McmU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "palabras_tilde = []\n",
        "\n",
        "for comentario in comentarios_sin_saltos:\n",
        "    palabras_tilde.extend(re.findall(r'\\b\\w*[áéíóú]\\w*\\b', comentario, re.IGNORECASE))\n",
        "\n",
        "imprimir_resultado(palabras_tilde,3)"
      ],
      "metadata": {
        "id": "nZZ5zKUOMeGD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c843696-2c0c-437b-f519-1a5f7b87b614"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+--------+--------+\n",
            "| Café | fiancé | puréed |\n",
            "+------+--------+--------+\n",
            "Se identificaron un total de \u001b[32m\u001b[1m3\u001b[0m palabras\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   **Pregunta 6.**  \n",
        "\n",
        "Busca e imprime todas las cantidades numéricas monetarias, enteras o con decimales, que inician con el símbolo $\\$$.\n",
        "\n",
        "Indica cuántos resultados obtuviste."
      ],
      "metadata": {
        "id": "ZmPiAI82Mfb3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cantidades_monetarias = []\n",
        "\n",
        "for comentario in comentarios_sin_saltos:\n",
        "    cantidades_monetarias.extend(re.findall(r'\\$\\s*(\\d+(?:\\.\\d+)?)', comentario))\n",
        "\n",
        "imprimir_resultado(cantidades_monetarias,4)"
      ],
      "metadata": {
        "id": "6vhe9-Y-MhL9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62e7dc95-6905-4c59-cf43-0c4464dfb9db"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----+----+-------+\n",
            "| 11.99 | 12 | 17 | 20    |\n",
            "+-------+----+----+-------+\n",
            "|  3    | 35 |  4 |  7.85 |\n",
            "+-------+----+----+-------+\n",
            "Se identificaron un total de \u001b[32m\u001b[1m8\u001b[0m palabras\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   **Pregunta 7.**  \n",
        "\n",
        "Busca e imprime todas las palabras que sean variantes de la palabra \"love\", sin importar si incluyen mayúsculas o minúsculas, o la manera en que esté conjugada o alguna otra variación que se haga con dicha palabra.\n",
        "\n",
        "Indica cuántos resultados obtuviste."
      ],
      "metadata": {
        "id": "2j-HpvhwMhq2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "regex = re.compile(r'\\b[Ll]ov(?:e[sd]?|ing|er|ers|ely|able|es|ed|ing|ingly|esque|ish|less|ness|ful|ment|s|d)\\b', re.IGNORECASE)\n",
        "palabras_love = []\n",
        "\n",
        "for comentario in comentarios_sin_saltos:\n",
        "  palabras_love.extend(regex.findall(comentario))\n",
        "\n",
        "imprimir_resultado(palabras_love,6)"
      ],
      "metadata": {
        "id": "kqqyRChVMjol",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be49e53e-2132-4715-c893-89a8a88d263a"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+-------+--------+--------+-------+--------+\n",
            "| LOVED  | LOVED |  Love  |  Love  | Loved | Loved  |\n",
            "+--------+-------+--------+--------+-------+--------+\n",
            "|  love  | love  |  love  |  love  | love  |  love  |\n",
            "+--------+-------+--------+--------+-------+--------+\n",
            "|  love  | love  |  love  |  love  | love  |  love  |\n",
            "+--------+-------+--------+--------+-------+--------+\n",
            "|  love  | love  |  love  |  love  | love  | loved  |\n",
            "+--------+-------+--------+--------+-------+--------+\n",
            "| loved  | loved | loved  | loved  | loved | lovely |\n",
            "+--------+-------+--------+--------+-------+--------+\n",
            "| lovely | lover | lovers | lovers | loves | loving |\n",
            "+--------+-------+--------+--------+-------+--------+\n",
            "Se identificaron un total de \u001b[32m\u001b[1m36\u001b[0m palabras\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   **Pregunta 8.**  \n",
        "\n",
        "Busca e imprime todas las palabras, variantes de \"so\" y \"good\", que tengan dos o más \"o\" en \"so\" y 3 o más \"o\" en good.\n",
        "\n",
        "Indica cuántas encontraste.\n"
      ],
      "metadata": {
        "id": "Ctb-NTY3MkYG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "regex_so = re.compile(r'\\bso+o+\\b', re.IGNORECASE)\n",
        "regex_good = re.compile(r'\\bgo+o+od\\w*\\b', re.IGNORECASE)\n",
        "\n",
        "# Buscar y almacenar todas las variantes de \"so\" y \"good\"\n",
        "palabras_so = []\n",
        "palabras_good = []\n",
        "for comentario in comentarios_sin_saltos:\n",
        "    palabras_so.extend(regex_so.findall(comentario))\n",
        "    palabras_good.extend(regex_good.findall(comentario))\n",
        "\n",
        "imprimir_resultado(palabras_so,3)\n",
        "imprimir_resultado(palabras_good,1)"
      ],
      "metadata": {
        "id": "A8Nf3B_cMlqg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f07955ea-71f4-47fe-866e-0bf6f9573ee7"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-------+-------+\n",
            "| Sooooo  | soooo | soooo |\n",
            "+---------+-------+-------+\n",
            "| soooooo |       |       |\n",
            "+---------+-------+-------+\n",
            "Se identificaron un total de \u001b[32m\u001b[1m4\u001b[0m palabras\n",
            "+--------+\n",
            "| gooodd |\n",
            "+--------+\n",
            "Se identificaron un total de \u001b[32m\u001b[1m1\u001b[0m palabras\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   **Pregunta 9.**  \n",
        "\n",
        "Busca e imprime todas las palabras que tengan una longitud mayor estrictamente a 10 caracteres alfabéticos.\n",
        "\n",
        "No se consideran los signos de puntuación o caracteres especiales en la longitud de estas cadenas, solo caracteres alfabéticos en mayúsculas o minúsculas.\n",
        "\n",
        "Indica la cantidad de palabras encontradas.\n"
      ],
      "metadata": {
        "id": "hkak1opjMmlk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "regex = re.compile(r'\\b[a-zA-Z]{11,}\\b')\n",
        "palabras_largas = []\n",
        "for comentario in comentarios_sin_saltos:\n",
        "  palabras_largas.extend(regex.findall(comentario))\n",
        "\n",
        "imprimir_resultado(palabras_largas,7)"
      ],
      "metadata": {
        "id": "PYxdp3uhMoD0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5689bde-895a-43f3-ea78-692ad933445f",
        "collapsed": true
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+-----------------+----------------+----------------+----------------+----------------+-----------------+\n",
            "|   Disappointed    |  Disappointing  | ESTABLISHMENT  |  Furthermore   | INCONSIDERATE  |  Interesting   |  Mediterranean  |\n",
            "+-------------------+-----------------+----------------+----------------+----------------+----------------+-----------------+\n",
            "|    Outstanding    |  Philadelphia   |  Smashburger   | Unfortunately  | Unfortunately  | Unfortunately  |   Veggitarian   |\n",
            "+-------------------+-----------------+----------------+----------------+----------------+----------------+-----------------+\n",
            "| WAAAAAAyyyyyyyyyy | Wienerschnitzel | accommodations |  accordingly   |  acknowledged  |  acknowledged  |   anticipated   |\n",
            "+-------------------+-----------------+----------------+----------------+----------------+----------------+-----------------+\n",
            "|    beautifully    |   calligraphy   |  caterpillar   |  cheeseburger  |  cheeseburger  |  cheesecurds   |  circumstances  |\n",
            "+-------------------+-----------------+----------------+----------------+----------------+----------------+-----------------+\n",
            "|    combination    |   combination   |  comfortable   |  comfortable   |  compliments   |  connoisseur   |   considering   |\n",
            "+-------------------+-----------------+----------------+----------------+----------------+----------------+-----------------+\n",
            "|    considering    |   considering   |  considering   |  constructed   |  corporation   |  deliciously   |  descriptions   |\n",
            "+-------------------+-----------------+----------------+----------------+----------------+----------------+-----------------+\n",
            "|   deuchebaggery   |  disappointed   |  disappointed  |  disappointed  |  disappointed  |  disappointed  |  disappointed   |\n",
            "+-------------------+-----------------+----------------+----------------+----------------+----------------+-----------------+\n",
            "|   disappointed    |  disappointed   |  disappointed  |  disappointed  |  disappointed  |  disappointed  |  disappointed   |\n",
            "+-------------------+-----------------+----------------+----------------+----------------+----------------+-----------------+\n",
            "|   disappointed    |  disappointed   |  disappointed  |  disappointed  |  disappointed  | disappointing  |  disappointing  |\n",
            "+-------------------+-----------------+----------------+----------------+----------------+----------------+-----------------+\n",
            "|   disappointing   |  disappointing  | disappointment | disappointment | disappointment | disappointment | disapppointment |\n",
            "+-------------------+-----------------+----------------+----------------+----------------+----------------+-----------------+\n",
            "|    disgraceful    |  disrespected   |  disrespected  |  drastically   |  enthusiastic  | establishment  |  establishment  |\n",
            "+-------------------+-----------------+----------------+----------------+----------------+----------------+-----------------+\n",
            "|   establishment   |   exceptional   |  expectations  |  experienced   |  experiencing  | extraordinary  |   grandmother   |\n",
            "+-------------------+-----------------+----------------+----------------+----------------+----------------+-----------------+\n",
            "|    highlighted    |   hospitality   |  imagination   |  imaginative   |  immediately   |  immediately   |   immediately   |\n",
            "+-------------------+-----------------+----------------+----------------+----------------+----------------+-----------------+\n",
            "|    inexpensive    |   inexpensive   |  informative   |  ingredients   |  interesting   |  interesting   |   maintaining   |\n",
            "+-------------------+-----------------+----------------+----------------+----------------+----------------+-----------------+\n",
            "|   neighborhood    |  neighborhood   |  opportunity   |  opportunity   |  outrageously  |  outstanding   |   outstanding   |\n",
            "+-------------------+-----------------+----------------+----------------+----------------+----------------+-----------------+\n",
            "|    overwhelmed    |   overwhelmed   |  presentation  |  presentation  |  professional  |  professional  |   profiterole   |\n",
            "+-------------------+-----------------+----------------+----------------+----------------+----------------+-----------------+\n",
            "|  recommendation   | recommendation  | recommendation |  recommended   |  recommended   |  recommended   |  recommending   |\n",
            "+-------------------+-----------------+----------------+----------------+----------------+----------------+-----------------+\n",
            "|   relationship    |   replenished   |  reservation   |  restaurants   |  restaurants   |  restaurants   |  shawarrrrrrma  |\n",
            "+-------------------+-----------------+----------------+----------------+----------------+----------------+-----------------+\n",
            "|    suggestions    |   traditional   |  transcendant  |  unbelievable  |  unbelievably  |  undercooked   |   undercooked   |\n",
            "+-------------------+-----------------+----------------+----------------+----------------+----------------+-----------------+\n",
            "|   underwhelming   |  underwhelming  | unexperienced  | unfortunately  | unprofessional |  unsatisfying  |   ventilation   |\n",
            "+-------------------+-----------------+----------------+----------------+----------------+----------------+-----------------+\n",
            "|    vinaigrette    |                 |                |                |                |                |                 |\n",
            "+-------------------+-----------------+----------------+----------------+----------------+----------------+-----------------+\n",
            "Se identificaron un total de \u001b[32m\u001b[1m141\u001b[0m palabras\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   **Pregunta 10.**  \n",
        "\n",
        "Busca e imprime todas las palabras que inician con una letra mayúscula y terminan con una minúscula, pero que además no sea la primera palabra del comentario/string.\n",
        "\n",
        "Indica la cantidad de resultados obtenidos."
      ],
      "metadata": {
        "id": "ApjTNzSxMpDc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "patron_10 = r\"(?<!^)\\b[A-Z][a-z]*[a-z]\\b\"\n",
        "palabras_10 = []\n",
        "for comentario in comentarios_sin_saltos:\n",
        "  palabras_10.extend(re.findall(patron_10, comentario))\n",
        "\n",
        "imprimir_resultado(palabras_10,8)"
      ],
      "metadata": {
        "id": "Vb0ndRGAMqdL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "148138cc-ce12-4daf-ac94-ccef4d43636c"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+------------+---------------+-----------------+---------------+--------------+------------+--------------+\n",
            "|    After    |   After    |     After     |   Albondigas    |      All      |     All      |    All     |     All      |\n",
            "+-------------+------------+---------------+-----------------+---------------+--------------+------------+--------------+\n",
            "|    Also     |    Also    |     Also      |      Also       |     Also      |   Although   |    And     |     And      |\n",
            "+-------------+------------+---------------+-----------------+---------------+--------------+------------+--------------+\n",
            "|   Anyway    |   Anyway   |    Anyways    |       Are       |     Area      |     Aria     |     As     |      As      |\n",
            "+-------------+------------+---------------+-----------------+---------------+--------------+------------+--------------+\n",
            "|     As      |     As     |      At       |       At        |    Attack     |     Baba     |   Bachi    |    Bachi     |\n",
            "+-------------+------------+---------------+-----------------+---------------+--------------+------------+--------------+\n",
            "|    Back     |    Bad     |      Bad      |       Bar       |   Baseball    |     Bay      |    Bay     |     Bay      |\n",
            "+-------------+------------+---------------+-----------------+---------------+--------------+------------+--------------+\n",
            "|     Bay     |     Be     |     Bean      |    Bellagio     |     Best      |     Best     |    Best    |     Big      |\n",
            "+-------------+------------+---------------+-----------------+---------------+--------------+------------+--------------+\n",
            "|   Bisque    |   Bisque   |     Blah      |     Bloody      |     Both      |     Both     |  Bouchon   |    Breeze    |\n",
            "+-------------+------------+---------------+-----------------+---------------+--------------+------------+--------------+\n",
            "|  Brushfire  |   Buffet   |    Buffet     |    Buldogis     |     Bunch     |    Burger    | Burrittos  |   Bussell    |\n",
            "+-------------+------------+---------------+-----------------+---------------+--------------+------------+--------------+\n",
            "|     By      | Caballero  |    Caesar     |    Camelback    |     Cape      |    Carly     |   Cartel   |    Casino    |\n",
            "+-------------+------------+---------------+-----------------+---------------+--------------+------------+--------------+\n",
            "|   Chicken   |  Chicken   |    Chinese    |     Chinese     |   Chipotle    |  Christmas   |    Cibo    |    Classy    |\n",
            "+-------------+------------+---------------+-----------------+---------------+--------------+------------+--------------+\n",
            "|     Cod     |   Coffee   |     Come      |     Company     |    Costco     |    Cotta     |   Crema    |   Crystals   |\n",
            "+-------------+------------+---------------+-----------------+---------------+--------------+------------+--------------+\n",
            "|  Customer   |    Cute    |  Definitely   |    Delicious    |    Delight    |    Denny     |  Despite   | Disappointed |\n",
            "+-------------+------------+---------------+-----------------+---------------+--------------+------------+--------------+\n",
            "|     Dog     |    Dos     |    Drinks     |      Dylan      |   Edinburgh   |     Egg      |  Eggplant  |     Elk      |\n",
            "+-------------+------------+---------------+-----------------+---------------+--------------+------------+--------------+\n",
            "|   English   |    Eve     |     Every     |    Everyone     |  Everything   |  Excalibur   | Experience |    Filet     |\n",
            "+-------------+------------+---------------+-----------------+---------------+--------------+------------+--------------+\n",
            "|    Filet    | Firehouse  |    Flower     |     Flower      |     Food      |     Food     |    For     |     For      |\n",
            "+-------------+------------+---------------+-----------------+---------------+--------------+------------+--------------+\n",
            "|     For     |    For     |   Francisco   |    Frenchman    |    Fridays    |    Friend    |   Frozen   | Furthermore  |\n",
            "+-------------+------------+---------------+-----------------+---------------+--------------+------------+--------------+\n",
            "|   Ganoush   |    Give    |     Gold      |      Good       |     Good      |     Good     |   Gordon   |   Gourmet    |\n",
            "+-------------+------------+---------------+-----------------+---------------+--------------+------------+--------------+\n",
            "|    Great    |   Great    |     Great     |      Great      |     Great     |    Great     |   Great    |    Great     |\n",
            "+-------------+------------+---------------+-----------------+---------------+--------------+------------+--------------+\n",
            "|    Greek    |   Greek    |     Greek     |      Green      |     Grill     |   Gringos    |   Gyros    |      Ha      |\n",
            "+-------------+------------+---------------+-----------------+---------------+--------------+------------+--------------+\n",
            "|   Halibut   |    Han     |     Hard      |    Hawaiian     |      He       |      He      |   Heart    |    Heimer    |\n",
            "+-------------+------------+---------------+-----------------+---------------+--------------+------------+--------------+\n",
            "|    Hiro     |  Honestly  |     Host      |       Hot       |      Hot      |     Hot      |    How     |   However    |\n",
            "+-------------+------------+---------------+-----------------+---------------+--------------+------------+--------------+\n",
            "|   However   |   Hunan    |      Hut      |      Ians       |      If       |      If      |     If     |      If      |\n",
            "+-------------+------------+---------------+-----------------+---------------+--------------+------------+--------------+\n",
            "|     If      |     If     |      If       |       If        |      In       |      In      |     In     |      In      |\n",
            "+-------------+------------+---------------+-----------------+---------------+--------------+------------+--------------+\n",
            "|     In      |   Indian   |    Indian     |     Insults     |    Ironman    |      It      |     It     |      It      |\n",
            "+-------------+------------+---------------+-----------------+---------------+--------------+------------+--------------+\n",
            "|     It      |     It     |      It       |       It        |      It       |      It      |     It     |   Italian    |\n",
            "+-------------+------------+---------------+-----------------+---------------+--------------+------------+--------------+\n",
            "|   Italian   |  Jamaican  |   Japanese    |      Jeff       |     Jenni     |     Joey     |   Kabuki   |     Khao     |\n",
            "+-------------+------------+---------------+-----------------+---------------+--------------+------------+--------------+\n",
            "|    Large    |    Las     |    Lastly     |      Lemon      |     Level     |   Lobster    |  Lobster   |   Lobster    |\n",
            "+-------------+------------+---------------+-----------------+---------------+--------------+------------+--------------+\n",
            "|    Long     |   Lordy    |     Love      |      Love       |     Loved     |    Loved     |    Lox     |     Luke     |\n",
            "+-------------+------------+---------------+-----------------+---------------+--------------+------------+--------------+\n",
            "|  Macarons   |  Madison   |   Magazine    |      Magic      |     Main      |    Maine     |  Mandalay  |    Mango     |\n",
            "+-------------+------------+---------------+-----------------+---------------+--------------+------------+--------------+\n",
            "|    Maria    |    Mary    |      May      |      Maybe      | Mediterranean |    Mellow    |  Mexican   |   Mexican    |\n",
            "+-------------+------------+---------------+-----------------+---------------+--------------+------------+--------------+\n",
            "|   Mirage    |    Mmmm    |      Mom      |    Mushroom     |      My       |      My      |     My     |      My      |\n",
            "+-------------+------------+---------------+-----------------+---------------+--------------+------------+--------------+\n",
            "|     Nan     |  Needless  |   Needless    |      Never      |     Nice      |     Nice     |   Ninja    |      No      |\n",
            "+-------------+------------+---------------+-----------------+---------------+--------------+------------+--------------+\n",
            "|     No      |    Nobu    |     Noca      |      North      |      Not      |     Not      |    Not     |     Not      |\n",
            "+-------------+------------+---------------+-----------------+---------------+--------------+------------+--------------+\n",
            "|     Not     |    Now     |      Of       |       Oh        |      On       |      On      |     On     |      On      |\n",
            "+-------------+------------+---------------+-----------------+---------------+--------------+------------+--------------+\n",
            "|     On      |    One     |    Ordered    |      Otto       |      Our      |     Our      |    Our     |     Our      |\n",
            "+-------------+------------+---------------+-----------------+---------------+--------------+------------+--------------+\n",
            "| Outstanding |  Overall   |    Overall    |     Overall     |    Overall    |   Overall    |    Palm    |    Panna     |\n",
            "+-------------+------------+---------------+-----------------+---------------+--------------+------------+--------------+\n",
            "|  Paradise   |  Perfect   |    Perfect    |     Perhaps     |  Phenomenal   | Philadelphia |    Pho     |     Pho      |\n",
            "+-------------+------------+---------------+-----------------+---------------+--------------+------------+--------------+\n",
            "|     Pho     |    Pho     |    Phoenix    |     Phoenix     |    Phoenix    |   Phoenix    | Pineapple  |     Pita     |\n",
            "+-------------+------------+---------------+-----------------+---------------+--------------+------------+--------------+\n",
            "|    Pizza    |   Pizza    |     Place     |     Plater      |     Plus      |    Point     |    Poor    |    Prices    |\n",
            "+-------------+------------+---------------+-----------------+---------------+--------------+------------+--------------+\n",
            "|   Prices    |  Probably  |     Pros      |     Ramsey      |    Really     |     Rice     |    Rick    |   Risotto    |\n",
            "+-------------+------------+---------------+-----------------+---------------+--------------+------------+--------------+\n",
            "|    Rock     |    Roll    |     Sadly     |      Salad      |    Salads     |     Same     |    San     |     Sat      |\n",
            "+-------------+------------+---------------+-----------------+---------------+--------------+------------+--------------+\n",
            "| Scottsdale  |    Seat    |   Seriously   |     Service     |    Service    |   Service    |    Shop    |  Similarly   |\n",
            "+-------------+------------+---------------+-----------------+---------------+--------------+------------+--------------+\n",
            "| Smashburger |    Soi     |     Sorry     |      Soups      |     Sour      |   Sprouts    |  Standard  |    Stars     |\n",
            "+-------------+------------+---------------+-----------------+---------------+--------------+------------+--------------+\n",
            "|    Steak    |   Steak    |   Steiners    |      Steve      |    Stopped    |    Strike    |   Strip    |    Strip     |\n",
            "+-------------+------------+---------------+-----------------+---------------+--------------+------------+--------------+\n",
            "|   Subway    |   Subway   |    Subway     |       Sun       |    Sunday     |    Sushi     |    Taco    |    Tasty     |\n",
            "+-------------+------------+---------------+-----------------+---------------+--------------+------------+--------------+\n",
            "|    Thai     |    Thai    |     Thai      |      Thai       |     Thai      |     That     |    That    |     The      |\n",
            "+-------------+------------+---------------+-----------------+---------------+--------------+------------+--------------+\n",
            "|     The     |    The     |      The      |       The       |      The      |     The      |    The     |     The      |\n",
            "+-------------+------------+---------------+-----------------+---------------+--------------+------------+--------------+\n",
            "|     The     |    The     |      The      |       The       |      The      |     The      |    The     |     The      |\n",
            "+-------------+------------+---------------+-----------------+---------------+--------------+------------+--------------+\n",
            "|     The     |    The     |      The      |       The       |      The      |     The      |    The     |     The      |\n",
            "+-------------+------------+---------------+-----------------+---------------+--------------+------------+--------------+\n",
            "|     The     |    The     |      The      |       The       |      The      |     The      |    The     |     The      |\n",
            "+-------------+------------+---------------+-----------------+---------------+--------------+------------+--------------+\n",
            "|     The     |    The     |      The      |       The       |      The      |     The      |    The     |     The      |\n",
            "+-------------+------------+---------------+-----------------+---------------+--------------+------------+--------------+\n",
            "|     The     |    The     |      The      |       The       |      The      |     The      |    The     |     The      |\n",
            "+-------------+------------+---------------+-----------------+---------------+--------------+------------+--------------+\n",
            "|     The     |    The     |      The      |       The       |      The      |     The      |    The     |    Their     |\n",
            "+-------------+------------+---------------+-----------------+---------------+--------------+------------+--------------+\n",
            "|    Then     |    Then    |     There     |      They       |     They      |     They     |    They    |     They     |\n",
            "+-------------+------------+---------------+-----------------+---------------+--------------+------------+--------------+\n",
            "|    They     |    They    |     Third     |      This       |     This      |     This     |    This    |     This     |\n",
            "+-------------+------------+---------------+-----------------+---------------+--------------+------------+--------------+\n",
            "|    This     |    This    |     This      |      This       |     This      |     This     |    This    |    Thumbs    |\n",
            "+-------------+------------+---------------+-----------------+---------------+--------------+------------+--------------+\n",
            "|    Thus     | Tigerlilly |      To       |       To        |      To       |    Toast     |    Took    |    Total     |\n",
            "+-------------+------------+---------------+-----------------+---------------+--------------+------------+--------------+\n",
            "|     Try     |   Tucson   | Unfortunately |  Unfortunately  | Unfortunately |      Up      |   Valley   |    Valley    |\n",
            "+-------------+------------+---------------+-----------------+---------------+--------------+------------+--------------+\n",
            "|    Vegas    |   Vegas    |     Vegas     |      Vegas      |     Vegas     |    Vegas     |   Vegas    |    Vegas     |\n",
            "+-------------+------------+---------------+-----------------+---------------+--------------+------------+--------------+\n",
            "|    Vegas    |   Vegas    |     Vegas     |      Vegas      |     Vegas     |    Vegas     |   Vegas    |    Vegas     |\n",
            "+-------------+------------+---------------+-----------------+---------------+--------------+------------+--------------+\n",
            "|    Vegas    |   Vegas    |     Vegas     |      Vegas      |     Vegas     |    Vegas     | Vegetarian | Veggitarian  |\n",
            "+-------------+------------+---------------+-----------------+---------------+--------------+------------+--------------+\n",
            "|    Very     |    Very    |     Very      |      Very       |     Very      |     Very     |    Very    |    Voodoo    |\n",
            "+-------------+------------+---------------+-----------------+---------------+--------------+------------+--------------+\n",
            "|     We      |     We     |      We       |       We        |     Went      |     What     |    What    |     When     |\n",
            "+-------------+------------+---------------+-----------------+---------------+--------------+------------+--------------+\n",
            "|    When     |    When    |     When      | Wienerschnitzel |     Wife      |     Will     |   Worse    |    Worst     |\n",
            "+-------------+------------+---------------+-----------------+---------------+--------------+------------+--------------+\n",
            "|    Yama     |    Yeah    |    Yelpers    |       You       |     Your      |              |            |              |\n",
            "+-------------+------------+---------------+-----------------+---------------+--------------+------------+--------------+\n",
            "Se identificaron un total de \u001b[32m\u001b[1m517\u001b[0m palabras\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   **Pregunta 11.**  \n",
        "\n",
        "Busca e imprime la secuencia de dos o más palabras que están separadas por un guion, \"-\", sin que tengan espacios en blanco entre ellas.\n",
        "\n",
        "Por ejemplo \"Go-Kart\" sería válido, pero \"Go  -Kart\" o \"Go  -  Kart\" no lo serían.\n",
        "\n",
        "Indica la cantidad de resultados obtenidos."
      ],
      "metadata": {
        "id": "u7nfm4KhMrNW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "patron_11 = r\"\\b\\w+-\\w+\\b\"\n",
        "palabras_11 = []\n",
        "for comentario in comentarios_sin_saltos:\n",
        "  palabras_11.extend(re.findall(patron_11, comentario))\n",
        "\n",
        "imprimir_resultado(palabras_11,3)"
      ],
      "metadata": {
        "id": "OwU-a7eGMsub",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3fe7192-7521-4fa2-c10c-a5dd5051cf65"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------+--------------------+----------------+\n",
            "| High-quality |   Service-check    |  been-stepped  |\n",
            "+--------------+--------------------+----------------+\n",
            "|  flat-lined  |   golden-crispy    |   hands-down   |\n",
            "+--------------+--------------------+----------------+\n",
            "|    in-and    |      in-house      |    low-key     |\n",
            "+--------------+--------------------+----------------+\n",
            "| multi-grain  |     must-stop      |  non-customer  |\n",
            "+--------------+--------------------+----------------+\n",
            "|  non-fancy   |      over-hip      |  over-priced   |\n",
            "+--------------+--------------------+----------------+\n",
            "|  over-whelm  |      sit-down      |    sub-par     |\n",
            "+--------------+--------------------+----------------+\n",
            "|    to-go     | tracked-everywhere | under-services |\n",
            "+--------------+--------------------+----------------+\n",
            "Se identificaron un total de \u001b[32m\u001b[1m21\u001b[0m palabras\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   **Pregunta 12.**  \n",
        "\n",
        "Busca e imprime todas las palabras que terminan en \"ing\" o \"ed\".\n",
        "\n",
        "Indica la cantidad de palabras que encontraste de cada una."
      ],
      "metadata": {
        "id": "DEIgl79HMthr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "patron_ing = r\"\\b\\w+ing\\b\"\n",
        "patron_ed = r\"\\b\\w+ed\\b\"\n",
        "\n",
        "# Buscar palabras que coincidan con cada patrón\n",
        "palabras_ing = []\n",
        "palabras_ed = []\n",
        "for comentario in comentarios_sin_saltos:\n",
        "  palabras_ing.extend(re.findall(patron_ing, comentario))\n",
        "  palabras_ed.extend(re.findall(patron_ed, comentario))\n",
        "\n",
        "imprimir_resultado(palabras_ing,8)"
      ],
      "metadata": {
        "id": "I4TSofBMMv9y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a45a302-a949-47ec-8d07-35fe0c534f2f"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------+---------------+---------------+---------------+---------------+---------------+---------------+------------+\n",
            "|    Coming    | Disappointing |  Everything   |  Everything   |  Everything   |  Everything   |  Everything   | Everything |\n",
            "+--------------+---------------+---------------+---------------+---------------+---------------+---------------+------------+\n",
            "| Interesting  |    Nothing    |  Outstanding  |    Paying     |    Pricing    |    amazing    |    amazing    |  amazing   |\n",
            "+--------------+---------------+---------------+---------------+---------------+---------------+---------------+------------+\n",
            "|   amazing    |    amazing    |    amazing    |    amazing    |    amazing    |    amazing    |    amazing    |  amazing   |\n",
            "+--------------+---------------+---------------+---------------+---------------+---------------+---------------+------------+\n",
            "|   amazing    |    amazing    |    amazing    |    amazing    |    amazing    |    amazing    |    amazing    |  amazing   |\n",
            "+--------------+---------------+---------------+---------------+---------------+---------------+---------------+------------+\n",
            "|   amazing    |    amazing    |    amazing    |    amazing    |    amazing    |   annoying    |   anything    |  anything  |\n",
            "+--------------+---------------+---------------+---------------+---------------+---------------+---------------+------------+\n",
            "|  appalling   |   appealing   |   arriving    |    asking     |     being     |     being     |     being     |   being    |\n",
            "+--------------+---------------+---------------+---------------+---------------+---------------+---------------+------------+\n",
            "|    being     |     being     |     being     |     being     |     being     |     being     |     being     |   being    |\n",
            "+--------------+---------------+---------------+---------------+---------------+---------------+---------------+------------+\n",
            "|    being     |     being     |    boring     |     bring     |     bring     |     bring     |     bring     |   bring    |\n",
            "+--------------+---------------+---------------+---------------+---------------+---------------+---------------+------------+\n",
            "|   building   |   building    |    buying     |    caring     |   changing    |   charming    |   climbing    |   coming   |\n",
            "+--------------+---------------+---------------+---------------+---------------+---------------+---------------+------------+\n",
            "|    coming    |    coming     |    coming     |    coming     |    coming     |    coming     |    coming     |   coming   |\n",
            "+--------------+---------------+---------------+---------------+---------------+---------------+---------------+------------+\n",
            "| considering  |  considering  |  considering  |  considering  |    cooking    |   cramming    |    craving    |  dealing   |\n",
            "+--------------+---------------+---------------+---------------+---------------+---------------+---------------+------------+\n",
            "|   dealing    |  describing   |    dining     |    dining     |    dining     |    dining     |    dining     |   dining   |\n",
            "+--------------+---------------+---------------+---------------+---------------+---------------+---------------+------------+\n",
            "|    dining    |    dining     |    dipping    | disappointing | disappointing | disappointing | disappointing | disgusting |\n",
            "+--------------+---------------+---------------+---------------+---------------+---------------+---------------+------------+\n",
            "|  disgusting  |     doing     |    drawing    |   dressing    |   dressing    |   dressing    |   drinking    |  dripping  |\n",
            "+--------------+---------------+---------------+---------------+---------------+---------------+---------------+------------+\n",
            "|   driving    |    during     |    during     |    during     |    during     |    during     |    eating     |   eating   |\n",
            "+--------------+---------------+---------------+---------------+---------------+---------------+---------------+------------+\n",
            "|    eating    |    eating     |    eating     |    eating     |    eating     |    eating     |    editing    |  evening   |\n",
            "+--------------+---------------+---------------+---------------+---------------+---------------+---------------+------------+\n",
            "|   evening    |  everything   |  everything   |  everything   |  everything   |  everything   |  everything   | exceeding  |\n",
            "+--------------+---------------+---------------+---------------+---------------+---------------+---------------+------------+\n",
            "| experiencing |    falling    |    feeling    |    feeling    |    filling    |    filling    |   flirting    | forgetting |\n",
            "+--------------+---------------+---------------+---------------+---------------+---------------+---------------+------------+\n",
            "|   freaking   |    fucking    |    getting    |    getting    |    getting    |    getting    |    getting    |  getting   |\n",
            "+--------------+---------------+---------------+---------------+---------------+---------------+---------------+------------+\n",
            "|   getting    |    getting    |    giving     |     going     |     going     |     going     |     going     |   going    |\n",
            "+--------------+---------------+---------------+---------------+---------------+---------------+---------------+------------+\n",
            "|    going     |     going     |     going     |     going     |     going     |     going     |     going     |   going    |\n",
            "+--------------+---------------+---------------+---------------+---------------+---------------+---------------+------------+\n",
            "|    going     |     going     |     going     |     going     |     going     |   handling    |   hankering   |   having   |\n",
            "+--------------+---------------+---------------+---------------+---------------+---------------+---------------+------------+\n",
            "|    hoping    |   including   |  interesting  |  interesting  |   inviting    |    judging    |    lacking    |  lacking   |\n",
            "+--------------+---------------+---------------+---------------+---------------+---------------+---------------+------------+\n",
            "|   lacking    |    letting    |   lighting    |    liking     |    living     |    looking    |    looking    |   loving   |\n",
            "+--------------+---------------+---------------+---------------+---------------+---------------+---------------+------------+\n",
            "| maintaining  |    making     |    missing    |    nothing    |    nothing    |    nothing    |    nothing    |  nothing   |\n",
            "+--------------+---------------+---------------+---------------+---------------+---------------+---------------+------------+\n",
            "|   nothing    |   ordering    |  outshining   |  outstanding  |  outstanding  |    playing    |    playing    | poisoning  |\n",
            "+--------------+---------------+---------------+---------------+---------------+---------------+---------------+------------+\n",
            "|  preparing   |   preparing   |    pricing    |   providing   |    putting    |    rating     |    raving     |  reading   |\n",
            "+--------------+---------------+---------------+---------------+---------------+---------------+---------------+------------+\n",
            "| recommending |   redeeming   |  refreshing   |   returning   |   reviewing   |  revisiting   |   rotating    |  running   |\n",
            "+--------------+---------------+---------------+---------------+---------------+---------------+---------------+------------+\n",
            "|   running    |    running    |    running    |   satifying   |  satisfying   |  satisfying   |    saving     |   saying   |\n",
            "+--------------+---------------+---------------+---------------+---------------+---------------+---------------+------------+\n",
            "|  seasoning   |    seating    |    seating    |    seating    |    serving    |    serving    |    setting    |  setting   |\n",
            "+--------------+---------------+---------------+---------------+---------------+---------------+---------------+------------+\n",
            "|   setting    |   shopping    |    sitting    |    sitting    |   something   |   something   |   something   | something  |\n",
            "+--------------+---------------+---------------+---------------+---------------+---------------+---------------+------------+\n",
            "|   sporting   |    spring     |   starving    |    staying    |    staying    |    talking    |     thing     |   thing    |\n",
            "+--------------+---------------+---------------+---------------+---------------+---------------+---------------+------------+\n",
            "|    thing     |     thing     |     thing     |     thing     |     thing     |     thing     |   thinking    |   trying   |\n",
            "+--------------+---------------+---------------+---------------+---------------+---------------+---------------+------------+\n",
            "|    trying    | underwhelming | underwhelming | unsatisfying  |   upgrading   |   venturing   |    waiting    |  waiting   |\n",
            "+--------------+---------------+---------------+---------------+---------------+---------------+---------------+------------+\n",
            "|   waiting    |    waiting    |    waiting    |    wasting    |    wasting    |    working    |    writing    |            |\n",
            "+--------------+---------------+---------------+---------------+---------------+---------------+---------------+------------+\n",
            "Se identificaron un total de \u001b[32m\u001b[1m279\u001b[0m palabras\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "imprimir_resultado(palabras_ed,9)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LyBhzb9GDchv",
        "outputId": "b02112f2-b186-4b3e-ff9d-b170e3035910"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------+--------------+--------------+--------------+--------------+--------------+---------------+--------------+--------------+\n",
            "|    Based     |    Cooked    | Disappointed |    Loved     |    Loved     |   Ordered    |    Ordered    |   Ordered    |  Overpriced  |\n",
            "+--------------+--------------+--------------+--------------+--------------+--------------+---------------+--------------+--------------+\n",
            "|   Stopped    |   Stopped    |    Tasted    |    Tried     |    Waited    |    Waited    | acknowledged  | acknowledged |    added     |\n",
            "+--------------+--------------+--------------+--------------+--------------+--------------+---------------+--------------+--------------+\n",
            "|    added     | anticipated  |   arrived    |   arrived    |   arrived    |    asked     |     asked     |    asked     |    asked     |\n",
            "+--------------+--------------+--------------+--------------+--------------+--------------+---------------+--------------+--------------+\n",
            "|    asked     |   attached   |   avoided    |    boiled    |    burned    |   charged    |    cheated    |   checked    |   checked    |\n",
            "+--------------+--------------+--------------+--------------+--------------+--------------+---------------+--------------+--------------+\n",
            "|   checked    |   claimed    |    closed    | constructed  |  contained   |    cooked    |    cooked     |    cooked    |    cooked    |\n",
            "+--------------+--------------+--------------+--------------+--------------+--------------+---------------+--------------+--------------+\n",
            "|    cooked    |    cooked    |   covered    |   decided    |   decided    |  decorated   |   decorated   |  dedicated   |   desired    |\n",
            "+--------------+--------------+--------------+--------------+--------------+--------------+---------------+--------------+--------------+\n",
            "| disappointed | disappointed | disappointed | disappointed | disappointed | disappointed | disappointed  | disappointed | disappointed |\n",
            "+--------------+--------------+--------------+--------------+--------------+--------------+---------------+--------------+--------------+\n",
            "| disappointed | disappointed | disappointed | disappointed | disappointed | disappointed | disappointed  | disappointed | disappointed |\n",
            "+--------------+--------------+--------------+--------------+--------------+--------------+---------------+--------------+--------------+\n",
            "|  disgusted   | disrespected | disrespected |   dreamed    |   drenched   |   dressed    |     dried     |   dropped    |    dusted    |\n",
            "+--------------+--------------+--------------+--------------+--------------+--------------+---------------+--------------+--------------+\n",
            "|    ended     |    ended     |   enjoyed    |   enjoyed    |   enjoyed    |   enjoyed    |    enjoyed    |    ensued    |   expanded   |\n",
            "+--------------+--------------+--------------+--------------+--------------+--------------+---------------+--------------+--------------+\n",
            "|   expected   |   expected   |   expected   | experienced  |     eyed     |   figured    |   flavored    |   focused    |   focused    |\n",
            "+--------------+--------------+--------------+--------------+--------------+--------------+---------------+--------------+--------------+\n",
            "|    fried     |    fried     |    fried     |    fried     |    fried     |  frustrated  |    greeted    |   greeted    |   grilled    |\n",
            "+--------------+--------------+--------------+--------------+--------------+--------------+---------------+--------------+--------------+\n",
            "|   grilled    |   grossed    |   grossed    |    handed    |   handled    |   happened   |     hated     |    helped    |    helped    |\n",
            "+--------------+--------------+--------------+--------------+--------------+--------------+---------------+--------------+--------------+\n",
            "| highlighted  |    hooked    |  humiliated  |     iced     |   ignored    |   imagined   |   impressed   |  impressed   |  impressed   |\n",
            "+--------------+--------------+--------------+--------------+--------------+--------------+---------------+--------------+--------------+\n",
            "|  impressed   |  impressed   |  impressed   |  impressed   |  impressed   |   included   |   inspired    |   insulted   |   insulted   |\n",
            "+--------------+--------------+--------------+--------------+--------------+--------------+---------------+--------------+--------------+\n",
            "|   insulted   |    lacked    |    lacked    |    lacked    |    liked     |    liked     |     liked     |   limited    |    lined     |\n",
            "+--------------+--------------+--------------+--------------+--------------+--------------+---------------+--------------+--------------+\n",
            "|    listed    |    lived     |   located    |    looked    |    looked    |    loved     |     loved     |    loved     |    loved     |\n",
            "+--------------+--------------+--------------+--------------+--------------+--------------+---------------+--------------+--------------+\n",
            "|    loved     |    loved     |   managed    |    melted    |    missed    |    mixed     |   mortified   |     need     |     need     |\n",
            "+--------------+--------------+--------------+--------------+--------------+--------------+---------------+--------------+--------------+\n",
            "|     need     |    needed    |    needed    |   offered    |    opened    |   opposed    |    ordered    |   ordered    |   ordered    |\n",
            "+--------------+--------------+--------------+--------------+--------------+--------------+---------------+--------------+--------------+\n",
            "|   ordered    |   ordered    |   ordered    |   ordered    |   ordered    |   ordered    |    ordered    |   ordered    |  overcooked  |\n",
            "+--------------+--------------+--------------+--------------+--------------+--------------+---------------+--------------+--------------+\n",
            "|  overcooked  |  overpriced  |  overpriced  |  overpriced  |  overpriced  |  overpriced  |  overwhelmed  | overwhelmed  |    owned     |\n",
            "+--------------+--------------+--------------+--------------+--------------+--------------+---------------+--------------+--------------+\n",
            "|    packed    |    passed    |    passed    |  performed   |   perpared   |  petrified   |    placed     |   pleased    |   pleased    |\n",
            "+--------------+--------------+--------------+--------------+--------------+--------------+---------------+--------------+--------------+\n",
            "|   pleased    |    poured    |   powdered   |   prepared   |    priced    |    priced    |    priced     |  privileged  |  proclaimed  |\n",
            "+--------------+--------------+--------------+--------------+--------------+--------------+---------------+--------------+--------------+\n",
            "|   provided   |    pulled    |    pulled    |    pulled    |    puréed    |  qualified   |     rated     |    rated     |   realized   |\n",
            "+--------------+--------------+--------------+--------------+--------------+--------------+---------------+--------------+--------------+\n",
            "|   realized   |   received   |   received   |   received   | recommended  | recommended  |  recommended  |     red      |     red      |\n",
            "+--------------+--------------+--------------+--------------+--------------+--------------+---------------+--------------+--------------+\n",
            "|  refrained   |   refried    |   refused    |   reheated   |   relaxed    |  relocated   |   reminded    | replenished  |  requested   |\n",
            "+--------------+--------------+--------------+--------------+--------------+--------------+---------------+--------------+--------------+\n",
            "|   returned   |    ripped    |    ripped    |   roasted    |   roasted    |    rolled    |    rushed     |  satisfied   |   screwed    |\n",
            "+--------------+--------------+--------------+--------------+--------------+--------------+---------------+--------------+--------------+\n",
            "|   seasoned   |   seasoned   |    seated    |    seated    |    seated    |    seated    |    seated     |    seemed    |    seemed    |\n",
            "+--------------+--------------+--------------+--------------+--------------+--------------+---------------+--------------+--------------+\n",
            "|    served    |    served    |    served    |    served    |    served    |    served    |    shocked    |    showed    |    sliced    |\n",
            "+--------------+--------------+--------------+--------------+--------------+--------------+---------------+--------------+--------------+\n",
            "|    sliced    |   smeared    |   smelled    |   started    |   started    |    stayed    |    stayed     |   stepped    |   stepped    |\n",
            "+--------------+--------------+--------------+--------------+--------------+--------------+---------------+--------------+--------------+\n",
            "|   stuffed    |   stuffed    |    sucked    |    sucked    |    sucked    |    sucked    |   supposed    |  surprised   |   tailored   |\n",
            "+--------------+--------------+--------------+--------------+--------------+--------------+---------------+--------------+--------------+\n",
            "|    tasted    |    tasted    |    tasted    |    tasted    |    tasted    |    tasted    |    tasted     |   thrilled   |   toasted    |\n",
            "+--------------+--------------+--------------+--------------+--------------+--------------+---------------+--------------+--------------+\n",
            "|   toasted    |   touched    |   tracked    |   treated    |   treated    |   treated    |     tried     |    tried     |    tried     |\n",
            "+--------------+--------------+--------------+--------------+--------------+--------------+---------------+--------------+--------------+\n",
            "|    tried     |    tried     |    tried     |   trimmed    | undercooked  | undercooked  | unexperienced |  uninspired  |  untoasted   |\n",
            "+--------------+--------------+--------------+--------------+--------------+--------------+---------------+--------------+--------------+\n",
            "|  unwrapped   |   uploaded   |     used     |     used     |     used     |   visited    |    vomited    |    voted     |    waited    |\n",
            "+--------------+--------------+--------------+--------------+--------------+--------------+---------------+--------------+--------------+\n",
            "|    waited    |    waited    |    waited    |    waited    |    waited    |    waited    |    waited     |    walked    |    walked    |\n",
            "+--------------+--------------+--------------+--------------+--------------+--------------+---------------+--------------+--------------+\n",
            "|    walked    |    wanted    |    wanted    |    wanted    |    wasted    |   watched    |    watched    |   watched    |   watered    |\n",
            "+--------------+--------------+--------------+--------------+--------------+--------------+---------------+--------------+--------------+\n",
            "|  witnessed   |   wrapped    |              |              |              |              |               |              |              |\n",
            "+--------------+--------------+--------------+--------------+--------------+--------------+---------------+--------------+--------------+\n",
            "Se identificaron un total de \u001b[32m\u001b[1m335\u001b[0m palabras\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Parte 3. Proceso de limpieza.**"
      ],
      "metadata": {
        "id": "70StdqAZa9E9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   **Pregunta 13.**  \n",
        "\n",
        "Ahora realiza un proceso de limpieza del corpus que incluya los siguientes procesos:\n",
        "\n",
        "*   Solo se deben considerar caracteres alfabéticos. Es decir, se eliminan todos los signos de puntuación y caracteres especiales.\n",
        "*   Todos los caracteres alfabéticos se transforman a minúsculas.\n",
        "*   Se deben eliminar todos los espacios en blanco adicionales que se puedan encontrar en cada comentario.\n",
        "\n",
        "Al finalizar dicho proceso de limpieza, imprime el resultado de los primeros 10 comentarios resultantes.\n",
        "   \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xaDUFXHrMvX2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def limpiar_texto(texto):\n",
        "    comentarios_limpios = []\n",
        "    for comentario in texto:\n",
        "        comentario_limpio = re.sub(r'[^a-zA-Z\\s]', '', comentario)  # Solo mantener caracteres alfabéticos (y espacios)\n",
        "        comentario_limpio = comentario_limpio.lower()               # Convertir a minúsculas\n",
        "        comentario_limpio = re.sub(r'\\s+', ' ', comentario_limpio)  # Eliminar espacios en blanco adicionales\n",
        "        comentarios_limpios.append(comentario_limpio)\n",
        "    return comentarios_limpios\n",
        ""
      ],
      "metadata": {
        "id": "K3kQzPOPMx0w"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "comentario_limpio = limpiar_texto(comentarios_sin_saltos)\n",
        "\n",
        "for comentario in comentario_limpio[:10]:\n",
        "    print(comentario)"
      ],
      "metadata": {
        "id": "mYEDlHSFMyJN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4021b08-2bbd-48c4-d9b2-113db7a10e4b"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "wow loved this place\n",
            "crust is not good\n",
            "not tasty and the texture was just nasty\n",
            "stopped by during the late may bank holiday off rick steve recommendation and loved it\n",
            "the selection on the menu was great and so were the prices\n",
            "now i am getting angry and i want my damn pho\n",
            "honeslty it didnt taste that fresh\n",
            "the potatoes were like rubber and you could tell they had been made up ahead of time being kept under a warmer\n",
            "the fries were great too\n",
            "a great touch\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   **Pregunta 14.**  \n",
        "\n",
        "Con el resultado de la limpieza obtenido en la pregunta anterior, realiza ahora un proceso de tokenización por palabras del corpus.\n",
        "\n",
        "Es decir, al final de este proceso de tokenización, debes tener como resultado una lista de listas, donde cada comentario estará tokenizado por palabras.\n",
        "\n",
        "Al terminar calcula el total de tokens obtenido en todo el corpus."
      ],
      "metadata": {
        "id": "WZwEhg2lUSAX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "comentarios_tokenizados = [comentario.split() for comentario in comentario_limpio]\n",
        "total_tokens = sum(len(comentario) for comentario in comentarios_tokenizados)\n",
        "\n",
        "print(f\"Total de tokens: {total_tokens}\")"
      ],
      "metadata": {
        "id": "kbAL9-v0V-jx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f3d7e0a-fa18-4ed2-ee02-af6dcfa698cd"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total de tokens: 10777\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   **Pregunta 15.**  \n",
        "\n",
        "Finalmente, en este ejercicio definiremos nuestro conjunto de palabras \"stopwords\", las cuales deberás eliminar de todo el corpus.\n",
        "\n",
        "Recuerda que ejemplos de stopwords son artículos, adverbios, conectivos, etcétera, que tienen frecuencias de aparición muy altas en cualquier documento, pero que no brindan mucho significado en cuanto al significado de un enunciado.\n",
        "\n",
        "Con base a la lista de stopwords que se te proporciona, realiza un proceso de limpieza eliminando todas estas palabras del corpus obtenido en el ejercicio anterior.\n",
        "\n",
        "Obtener cuántos tokens/palabras quedan finalmente en todo el corpus.\n",
        "\n",
        "Obtener cuántos de estos tokens/palabras son diferentes, es decir, cuántos tokens únicos tendrá lo que llamaremos más adelante nuestro vocabulario."
      ],
      "metadata": {
        "id": "EFeu0OJ7WDPD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Considera la siguiente lista como tu conjunto de stopwords:\n",
        "mis_stopwords = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'should', 'now', 'll']"
      ],
      "metadata": {
        "id": "6FP4FF3KXGxm"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "comentarios_sin_stopwords = []\n",
        "\n",
        "for comentario in comentarios_tokenizados:\n",
        "  comentarios_sin_stopwords.append([palabra for palabra in comentario if palabra not in mis_stopwords])\n",
        "\n",
        "total_tokens_restantes = sum(len(comentario) for comentario in comentarios_sin_stopwords)\n",
        "\n",
        "print(f\"Total de tokens restantes: \\033[32m\\033[1m{total_tokens_restantes}\\033[0m\")\n",
        "\n",
        "# Calcular tokens únicos (vocabulario)\n",
        "tokens_unicos = set([palabra for comentario in comentarios_sin_stopwords for palabra in comentario])\n",
        "total_tokens_unicos = len(tokens_unicos)\n",
        "print(f\"Total de tokens únicos (vocabulario): \\033[32m\\033[1m{total_tokens_unicos}\\033[0m\")"
      ],
      "metadata": {
        "id": "CD8yjyq1ZrwY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "810a9d40-de11-4f50-955e-601eb5725b40"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total de tokens restantes: \u001b[32m\u001b[1m5776\u001b[0m\n",
            "Total de tokens únicos (vocabulario): \u001b[32m\u001b[1m1941\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   **Comentarios**\n",
        "\n",
        "Incluye finalmente tus comentarios de la actividad."
      ],
      "metadata": {
        "id": "NDbKkuxRbLoX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<< incluye aquí tus comentarios >>"
      ],
      "metadata": {
        "id": "o7fzbvqVbUGr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Fin de la Actividad de la semana 2.**"
      ],
      "metadata": {
        "id": "PHaKw_6Ldbaf"
      }
    }
  ]
}